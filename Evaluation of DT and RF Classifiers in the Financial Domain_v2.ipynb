{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# BSc Thesis: Evaluation of Decision Tree and Random Forest Classifiers in the Finance Domain"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Chapters 0 to 3 provide functions, Chapter 4 combines these functions for convenient usage, Chapter 6 contains final evaluations\n## Table of Contents\n0. Preparation\n1. Data Preparation Stage\n2. Classification Stage\n3. Evaluation Stage\n4. Putting it all together\n5. Visualisation\n6. Final Evaluations"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# 0 | Preparation\n- Import libraries and define constant variables"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Imports"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Data manipulation and arrays\nimport pandas as pd\nimport numpy as np\n\n# Machine learning\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, learning_curve, GridSearchCV, RandomizedSearchCV\nfrom sklearn import metrics\n\n# Plottig\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom IPython.display import display",
      "execution_count": 119,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Constant variables"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define filenames for technology stock .CSV datasets\nAAPL_DATA = \"Datasets/Kaggle_SnP500_AAPL_2013-2018.csv\"\nAMZN_DATA = \"Datasets/Kaggle_SnP500_AMZN_2013-2018.csv\"\nCSCO_DATA = \"Datasets/Kaggle_SnP500_CSCO_2013-2018.csv\"\nGE_DATA = \"Datasets/Kaggle_SnP500_GE_2013-2018.csv\"\nGOOGL_DATA = \"Datasets/Kaggle_SnP500_GOOGL_2013-2018.csv\"\nHP_DATA = \"Datasets/Kaggle_SnP500_HP_2013-2018.csv\"\nIBM_DATA = \"Datasets/Kaggle_SnP500_IBM_2013-2018.csv\"\nINTC_DATA = \"Datasets/Kaggle_SnP500_INTC_2013-2018.csv\"\nMSFT_DATA = \"Datasets/Kaggle_SnP500_MSFT_2013-2018.csv\"\nWU_DATA = \"Datasets/Kaggle_SnP500_WU_2013-2018.csv\"\nXRX_DATA = \"Datasets/Kaggle_SnP500_XRX_2013-2018.csv\"\nTECH_GROUP = [AAPL_DATA, AMZN_DATA, CSCO_DATA, GE_DATA, GOOGL_DATA, HP_DATA, IBM_DATA, INTC_DATA, MSFT_DATA, WU_DATA, XRX_DATA]\n\n# Define time horizons to compare classification results for 1-day to 1-year predictions (approx. trading days)\nTIME_HORIZONS = [1, 5, 10, 20, 65, 250]\n\n# Make code reproducible by seeding random states\nRANDOM_SEED = 42",
      "execution_count": 145,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1 | Data Preparation Stage\n- Load data and adjust columns as needed\n- Extract features for technical analysis\n- Define class for later classification\n- Detect anomalies in the datasets\n- No feature selection needed as embedded in Decision Trees (DT) and Random Forests (RF)\n\n## 1.1 | Load Datasets\n- For an apples-to-apples comparison, technology companies are analyzed (idea: companies/stocks within an industry have similar drivers)\n- Selected stocks differ in price trends (upward- vs constant- vs downward trend)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_OHLC_data(filename=MSFT_DATA, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Loads basic stock data (date, name, open, high, low, close) from a given .CSV file and returns a corresponding DataFrame.\n    Unnecessary categorical columns are dropped, and necessary columns (e.g. month as number) are added.\n    \"\"\"\n    try:\n        df = pd.read_csv(filename)\n        if SAVE_FIG is True:\n            # Visualize loaded time series data if applicable\n            df[\"date\"] = pd.to_datetime(df[\"date\"])\n            df.plot(x=\"date\", y=\"close\", figsize=(12,6), legend=None)\n            plt.xlabel(\"Time [Year]\")\n            plt.ylabel(\"Price [daily closing price in USD]\")\n            plt.title(df[\"Name\"][0] + \"-Stock Data 2013 to 2018\");\n            plt.savefig(\"./Plots/\" + df[\"Name\"][0] + \"-Stock-Price-Plot.jpeg\")\n        \n        # Calculate base column for later class: future return of stock over given time horizon (e.g. this week's Monday to next week's Monday)\n        for horizon in time_horizons:\n            df[\"return_future_\" + str(horizon) + \"d\"] = (df[\"close\"].shift(-1*horizon)/df[\"close\"])-1\n        \n        # Convert date to numerical month to possibly detect cyclicality (e.g. christmas effect) in time series\n        df[\"month\"] = df[\"date\"].astype(\"datetime64[ns]\").dt.month\n        \n        if VERBOSE is True:\n            print(\"Loaded DataFrame has the following columns:\")\n            for col in df:\n                print(\"Column \\'\" + col + \"\\' with type\", type(df[col][0]), \", e.g.\", df[col][0])\n            print(\"df.head():\")\n            print(df.head())\n            \n        return df\n    except:\n        print(\"Error, failed to find or load OHLC data from file with name \\'\"\n              + filename + \"\\'. Please provide well-formed CSV file with OHLC stock data\")",
      "execution_count": 121,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# dfs_TECH_GROUP = []\n# for stock in TECH_GROUP:\n#     dfs_TECH_GROUP.append(load_OHLC_data(stock))",
      "execution_count": 122,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.2 | Extract Features\n- Common metrics for technical analysis are calculated to be later used as features\n- Suitable TA libary for more technical features: https://github.com/bukosabino/ta"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def extract_OHLC_features(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Extract common technical stock analysis features from given OHLC stock data for distinct time horizons\n    \"\"\"\n    # Calculate technical features for each time horizon\n    for horizon in time_horizons:\n        # Past return of stock over given time horizon (e.g. last week's Monday to this week's Monday)\n        df[\"return_past_\" + str(horizon) + \"d\"] = (df[\"close\"]/df[\"close\"].shift(horizon))-1\n        \n        # Implied volatility measured by standard deviation\n        df[\"volatility_\" + str(horizon) + \"d\"] = df[\"close\"].rolling(horizon).std()\n        \n        # Moving averages (ma)\n        df[\"ma_\" + str(horizon) + \"d\"] = df[\"close\"].rolling(horizon).mean()\n        \n        # Momentum (absolute change in price over past horizon)\n        df[\"momentum_\" + str(horizon) + \"d\"] = df[\"close\"].diff(horizon)\n        \n        # If needed, use TA library for: ewma, #Pivot Points, Bollinger bands, Supports and Resistances, etc.\n    \n    # OHLC average is used for stock price average of a given day\n    df[\"ohlc_avg\"] = df[[\"open\", \"high\", \"low\", \"close\"]].mean(axis=1)\n    \n    # Replace NaNs with zeroes\n    df = df.fillna(value=0)\n    return df",
      "execution_count": 123,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# for df in dfs_TECH_GROUP:\n#     df = extract_OHLC_features(df, TIME_HORIZONS)",
      "execution_count": 124,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## 1.3 | Anomaly Detection\n- Anomaly defined as: ABS(return_past_1d) > threshold=5% (default)\n- Such anomalies (5% threshold) occur in about 1.4% of instances for seven tech stock datasets"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "def detect_anomalies(df, threshold=0.05):\n    \"\"\"\n    Iterates through the DataFrame and prints out all dates where 1-day-return is greater than threshold=5% (default)\n    \"\"\"\n    if VERBOSE is True:\n        print(\"Detecting anomalies where abs(1-day-return)>\" + str(threshold*100) + \" % for \" + df[\"Name\"][0])\n\n    anomaly_counter = 0\n    for i in range(len(df)):\n        x = df[\"return_past_1d\"][i]\n        d = df[\"date\"][i]\n        if (abs(x) > threshold):\n            anomaly_counter = anomaly_counter + 1\n            if VERBOSE is True:\n                print(\"Anomaly: 1-day-return of \" + str(round(x * 100, 2)) + \"% on \" + d.strftime(\"%A, %d.%m.%Y\"))\n    return anomaly_counter",
      "execution_count": 125,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# anomaly_counter = 0\n# for df in dfs_TECH_GROUP:\n#     anomaly_counter = anomaly_counter + detect_anomalies(df, threshold=0.05)\n\n# print(\"anomaly_counter=\" + str(anomaly_counter) + \", or \" + str(round(anomaly_counter*100/(len(TECH_GROUP)*len(dfs_TECH_GROUP[0])), 2)) + \"% of instances\")",
      "execution_count": 126,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.4 | Define classes\n- This notebook evaluates DT and RF for stock recommendation (application no. 2 in thesis)\n- Classes are defined for each time horizon to enable for later comparisons\n- Base columns, on which classes are built, are removed to prevent illegal future-peeking features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def define_classes(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Create target column in df: 1 means 'Yes, investor should buy stock', 0 means 'No, investor should not buy stock'.\n    The assumed trading strategy here is, that the investor buy the stock on a given date and sells it after the horizon period.\n    Also removes illegal (future-peeking) columns\n    \"\"\"\n    for horizon in time_horizons:\n        base_column_name = \"return_future_\" + str(horizon) + \"d\"\n        class_name = \"class_\" + str(horizon) + \"d\"\n        \n        if class_name not in df.columns:\n            df[class_name] = np.where(df[base_column_name] > 0, 1, 0)\n        # Remove base column as it would be an illegal (future-peeking) feature\n        if base_column_name in df.columns:\n            df = df.drop(columns=[base_column_name])\n    return df",
      "execution_count": 127,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for i in range(0, len(dfs_TECH_GROUP)):\n#     dfs_TECH_GROUP[i] = define_classes(dfs_TECH_GROUP[i])",
      "execution_count": 128,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.5 | Check class balance\n- The two classes Yes (1) and No (0) should be balanced, else the evaluation technique must be adapted"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def check_class_balance(df, time_horizons=TIME_HORIZONS):\n    for horizon in time_horizons:\n        class_name = \"class_\" + str(horizon) + \"d\"\n        if VERBOSE is True:\n            print(df[class_name].value_counts())\n\n        plt.figure()\n        df[class_name].hist()\n        plt.xlabel(\"Klasse\")\n        plt.ylabel(\"Haeufigkeit\")\n        plt.title(df[\"Name\"][0] + \"-Klassen-Histogramm-\" + str(horizon) + \"d\")\n        if SAVE_FIG is True:\n            plt.savefig(\"./Plots/Class-Balance-Check/\" + df[\"Name\"][0] + \"-Class-Balance-Histogram-\" + str(horizon) + \"d.jpeg\")\n        plt.close()",
      "execution_count": 129,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for i in range(0, len(dfs_TECH_GROUP)):\n#     check_class_balance(dfs_TECH_GROUP[i])",
      "execution_count": 130,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# 2 | Classification Stage"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.1 | Define training and test sets\n- Seed RandomState-s uniformly to make algorithms reproducible"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train_test_split_data(df, train_size=0.7, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Generates training and testing set from a given DataFrame dataset.\n    Assumes last COUNT(time_horizons) column(s) in DataFrame are classes,\n    others are features.\n    Returns features (X) and targets (y) in training- and testing sets.\n    \"\"\"\n    # 1. Remove non-numerical features and save final feature names (e.g. for later matching to feature importances)\n    df = df.drop(columns=[\"Name\", \"date\"])\n    global g_feature_names\n    g_feature_names = df.columns.values.tolist()\n    if VERBOSE is True:\n        print(\"generate_train_test_data() -> features_names: \" + str(g_feature_names))\n        print(\"generate_train_test_data() -> len(features_names): \" + str(len(g_feature_names)))\n    \n    # Split DataFrame into features (X) and target (y)\n    X = df.iloc[:,:-1*len(time_horizons)]\n    y = df.iloc[:,-1*len(time_horizons):]\n    \n    # Use first (in same chronological order as time series) 70% to train and last 30% to test\n    split_index = int(len(X) * train_size)\n    if VERBOSE is True:\n        print(\"train_test_split_data() -> Split ist bei Index \" + str(split_index) + \" von \" + str(len(X)))\n    \n    X_train, X_test = X[:split_index], X[split_index:]\n    y_train, y_test = y[:split_index], y[split_index:]\n    return X_train, X_test, y_train, y_test",
      "execution_count": 131,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.2.1 | Hyperparameter tuning for decision tree"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tune_decision_tree(dt, X, y):\n    \"\"\"\n    Returns decision tree with best hyperparameters (out of all grid combinations)\n    \"\"\"\n    # 1. Create grid with all values that should be considered as hyperparameters\n    param_grid = {\"max_depth\": [3, 5, 7, 9, 11, 13, 15, 29],\n                      \"max_features\": [3, 5, 9 ,11, 13, 15, 29],\n                      \"min_samples_split\": [3, 5, 7, 10, 15, 29],\n                      \"min_samples_leaf\": [1, 3, 5, 7, 11, 29]}\n    # 2. Run GridSearch. Randomized version is much faster with a small loss in optimality\n    tscv = TimeSeriesSplit(n_splits=10)\n    # grid_search = GridSearchCV(dt, param_grid, cv=tscv)\n    grid_search = RandomizedSearchCV(dt, param_grid, cv=tscv, random_state=RANDOM_SEED)\n    grid_search.fit(X, y)\n\n    if VERBOSE is True:\n        print(\"tune_decision_tree() done.\")\n        print(\"Best score: \" + str(grid_search.best_score_ ))\n        print(\"Best params: \" + str(grid_search.best_params_ ))\n        print(\"Best estimator: \" + str(grid_search.best_estimator_))\n    \n    return grid_search.best_estimator_",
      "execution_count": 132,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(AAPL_DATA, horizon_index=2, train_size=1)\n# clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n# dt_new = tune_decision_tree(clf, X_train, y_train)\n# print(\"dt_new ist: \" + str(dt_new))",
      "execution_count": 133,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.2.2 | Hyperparameter tuning for random forest"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tune_random_forest(rf, X, y):\n    \"\"\"\n    Returns random forest with best hyperparameters (out of all grid combinations)\n    \"\"\"\n    # 1. Create grid with all values that should be considered as hyperparameters\n    param_grid = {\"max_depth\": [3, 7, 17],\n                        \"min_samples_leaf\": [3, 13, 23]}\n#     rf.set_params(n_estimators=100) # Set n_estimators afterwards to reduce searching time\n    # 2. Run GridSearch. Randomized version is much faster with a small loss in optimality\n    tscv = TimeSeriesSplit(n_splits=10)\n    # grid_search = GridSearchCV(dt, param_grid, cv=tscv)\n    grid_search = RandomizedSearchCV(rf, param_grid, cv=tscv, random_state=RANDOM_SEED, n_iter=9)\n    grid_search.fit(X, y)\n\n    if VERBOSE is True:\n        print(\"tune_random_forest() done.\")\n        print(\"Best score: \" + str(grid_search.best_score_ ))\n        print(\"Best params: \" + str(grid_search.best_params_ ))\n        print(\"Best estimator: \" + str(grid_search.best_estimator_))\n    \n    return grid_search.best_estimator_",
      "execution_count": 134,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(AAPL_DATA, horizon_index=2, train_size=1)\n# clf = RandomForestClassifier(n_estimators=5, random_state=RANDOM_SEED)\n# rf_new = tune_random_forest(clf, X_train, y_train)\n# print(\"rf_new ist: \" + str(rf_new))",
      "execution_count": 135,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3 | Evaluation Stage\n- Build confusion matrixes and calculate performance metrics or determine feature importances\n- Plot findings"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.1 | Evaluate classifier with Time Series CV"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def apply_tscv(clf, X, y, return_predictions=False, tuning=False, importances_only=False):\n    \"\"\"\n    Calculates either accuracy or feature importances of given classifier by Time Series Cross Validation (tscv)\n    \"\"\"\n    # Instantiate TSCV and result variables\n    tscv = TimeSeriesSplit(n_splits=10)\n    train_sizes_manual = []\n    test_sizes_manual = []\n    train_scores_manual = []\n    test_scores_manual = []\n    y_actuals_raw = []\n    y_predicteds_raw = []\n    \n    # Manually .fit() and .evaluate() data via TSCV (as learning_curve() does not seem to work with TSCV, e.g. train_sizes wrong)\n    epoch_counter = 0\n    for train_index, test_index in tscv.split(X):\n        # 1. Define indizes for training and testing\n        first_train_index = 0\n        last_train_index = train_index[-1]\n        first_test_index = test_index[0]\n        last_test_index = test_index[-1]\n        \n        global g_current_tscv_epoch\n        g_current_tscv_epoch = epoch_counter\n        if VERBOSE is True:\n            print(\"apply_tscv() -> Epoch=\" + str(epoch_counter) + \", train on indices=[\" + str(first_train_index) + \n                  \", \" + str(last_train_index) + \"], test on indices=[\" + str(first_test_index) + \", \" + str(last_test_index) +\n                  \"], \" + \"i.e. train_size=\", len(train_index), \", test_size=\", len(test_index))\n        train_sizes_manual.extend([len(train_index)])\n        test_sizes_manual.extend([len(test_index)])\n        \n        # 2. Create training and test sets\n        X_train = X[first_train_index:last_train_index]\n        X_test = X[first_test_index:last_test_index]\n        y_train = y[first_train_index:last_train_index]\n        y_test = y[first_test_index:last_test_index]\n        \n        # 3. Delegate calculation of feature importances, raw metrics (actuals, predicteds) or aggregated metrics (accuracy etc.)\n        if importances_only is True:\n            calculate_metrics(clf, X_train, X_test, y_train, y_test, return_predictions=return_predictions, tuning=tuning, importances_only=True)\n        elif return_predictions is True:\n            y_actual, y_predicted = calculate_metrics(clf, X_train, X_test, y_train, y_test, return_predictions=True, tuning=tuning, importances_only=False)\n            # 3a. Add raw values to return list\n            y_actuals_raw.extend(y_actual)\n            y_predicteds_raw.extend(y_predicted)\n        else:\n            train_score = calculate_metrics(clf, X_train, X_train, y_train, y_train, tuning=tuning, importances_only=importances_only)\n            test_score = calculate_metrics(clf, X_train, X_test, y_train, y_test, tuning=tuning, importances_only=importances_only)\n            # 3b. Add new scores to return list\n            train_scores_manual.extend([train_score])\n            test_scores_manual.extend([test_score])\n        \n        # 4. Increase epoch counter  \n        epoch_counter = epoch_counter + 1\n    \n    if VERBOSE is True and return_predictions is not True and importances_only is not True:\n        print(\"apply_tscv() -> train_sizes_manual=\" + str(train_sizes_manual) + \", test_sizes_manual: \" + str(test_sizes_manual))\n        print(\"apply_tscv() -> train_scores_manual=\" + str([round(e, 2) for e in train_scores_manual]))\n        print(\"apply_tscv() -> test_scores_manual=\" + str([round(e, 2) for e in test_scores_manual]))\n    \n    if importances_only is True:\n        return\n    elif return_predictions is True:\n        return y_actuals_raw, y_predicteds_raw\n    else:\n        return train_sizes_manual, train_scores_manual, test_scores_manual",
      "execution_count": 136,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.2 | Calculate metrics for given classifier's X-s and y-s"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def calculate_metrics(clf, X_train, X_test, y_train, y_test, return_predictions=False, tuning=False, importances_only=False):\n    \"\"\"\n    Calculates accuracy, precision, recall and f-measure for given classifier for given training and test data.\n    Only accepts y-DataFrames with one single column/one single horizon (as only one class is predicted).\n    return_predictions argument, if True, returns the raw TP/TN/FP/FN values, instead of aggregated metrics\n    \"\"\"\n    if not isinstance(y_train, pd.Series) or not isinstance(y_test, pd.Series):\n        print(\"y-DataFrames must have 1 column only.\")\n        return -1\n    \n    # 1. Fit and, if applicable, tune classifier to predict y-s\n    if tuning is True and isinstance(clf, type(DecisionTreeClassifier())):\n        clf = tune_decision_tree(clf, X_train, y_train)\n    elif tuning is True and isinstance(clf, type(RandomForestClassifier())):\n        clf = tune_random_forest(clf, X_train, y_train)\n        clf.set_params(n_estimators=100) # set after GridSearch to reduce searching time\n    else:\n        clf.fit(X_train, y_train)\n    \n    # 2a. Path A: Find and, if applicable, plot feature importances\n    if importances_only is True:\n        feature_importances = clf.feature_importances_\n        global g_feature_importances\n        g_feature_importances.append(feature_importances) # allow for global averaging (e.g. per horizon across all epochs)\n        \n        # First, find the top-10 features by importance\n        feature_ranking_indicies = np.argsort(feature_importances)\n        top10_features_indices = feature_ranking_indicies[-10:]\n        if VERBOSE is True:\n            print(\"Top1 feature:\" + str(g_feature_names[top10_features_indices[-1]]) + \" ->\" + str(feature_importances[top10_features_indices[-1]]))\n            print(\"Top2 feature:\" + str(g_feature_names[top10_features_indices[-2]]) + \" ->\" + str(feature_importances[top10_features_indices[-2]]))\n            print(\"Top3 feature:\" + str(g_feature_names[top10_features_indices[-3]]) + \" ->\" + str(feature_importances[top10_features_indices[-3]]))\n        \n        # Second, plot feature importances in horizontal bars\n        fig = plt.figure()\n        plt.title(\"Einflussgrade der Features, \" + str(g_classifier) + \", AAPL\")\n        plt.barh(y=[g_feature_names[i] for i in top10_features_indices], width=[feature_importances[i] for i in top10_features_indices])\n        fig.tight_layout()\n        if SAVE_FIG is True:\n            plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Top10Importances-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \n                        \"-Epoch\" + str(g_current_tscv_epoch)+ \"-\" + str(g_stock_ticker) + \".jpeg\")\n        plt.close()\n    \n    # 2b. Path B: Calculate and return metrics\n    else:\n        y_actual = y_test\n        y_predicted = clf.predict(X_test)\n        acc = metrics.accuracy_score(y_actual, y_predicted)\n        precision = metrics.precision_score(y_actual, y_predicted) if (1 in y_predicted and 1 in y_actual) else -1\n        recall = metrics.recall_score(y_actual, y_predicted) if (1 in y_predicted and 1 in y_actual) else -1\n        f_measure = metrics.f1_score(y_actual, y_predicted) if (precision > 0 or recall >0) else -1\n\n        if VERBOSE is True:\n            print(\"calculate_metrics() -> model=\" + str(type(clf)) + \", class=\" + y_train.name + \", acc=\" + str(round(acc, 2)) + \n                  \"prec=\" + str(precision) + \", recall=\" + str(precision) + \", f_score=\" + str(f_measure))\n\n        # 4. Return either raw metrics (actuals, predicteds) or aggregated metrics (accuracy etc.)\n        if return_predictions is True:\n            return y_actual, y_predicted\n        else:\n            return acc # TBD: return other metrics as well",
      "execution_count": 137,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 4 | Putting it all together (all above methods are called here in one single place)\n- Prepare, classify and evaluate datasets"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.1 | Summarizing function for loading & extracting vs not data & splitting for train vs test"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_train_test_data(filename=MSFT_DATA, extract_features=True, horizon_index=2, train_size=0.7):\n    \"\"\"\n    Serves as a one-stop-shop function for data loading incl. preparation so that classifier only relies on this single fucntion (and not many single functions) \n    \"\"\"\n    # 1. Load data from .CSV (1.1)\n    df = load_OHLC_data(filename)\n    stock_ticker = df[\"Name\"][0]\n    \n    if VERBOSE is True:\n        print(\"generate_train_test_data() for \" + str(stock_ticker))\n    \n    # 2. Extract features if applicable (1.2)\n    if extract_features is True:\n        df = extract_OHLC_features(df)\n    \n    # Detect anomalies (1.3) skipped\n    # 3. Define classes (1.4)\n    df = define_classes(df)\n    \n    # Check class balance (1.5) skipped\n    # 4. Split data into training and testing samples if applicable (or get 100% as training data to later apply CV)\n    X_train, X_test, y_train, y_test = train_test_split_data(df, train_size=train_size)\n    \n    # 5. Return one single class series, depending on the horizon_index provided\n    y_train = y_train[y_train.columns[horizon_index]]\n    y_test = y_test[y_test.columns[horizon_index]]\n    \n    return stock_ticker, X_train, X_test, y_train, y_test",
      "execution_count": 138,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(extract_features=False)\n# print(X_train.columns)\n# print(y_train.name)\n# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(extract_features=True)\n# print(X_train.columns)\n# print(y_train.name)",
      "execution_count": 139,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exkurs: Erkärung, warum der DT bei AAPL, Horizont-Index=5, Epochen1/2 nicht wächst (bzw Depth=0) -> Klasse hat nur positive 1er!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# s, X_train, X_test, y_train, y_test = generate_train_test_data(horizon_index=5, train_size=1)\n\n# print(\"len X_test: \" + str(len(X_test)))\n# print(\"len X_train: \" + str(len(X_train)))\n\n# print(\"len y_test: \" + str(len(y_test)))\n# print(\"len y_train: \" + str(len(y_train)))\n\n# apply_tscv(DecisionTreeClassifier(random_state=RANDOM_SEED), X_train, y_train, return_predictions=False, tuning=False, importances_only=True)",
      "execution_count": 140,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 5 | Visualization"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5.1 | Plot acuracy curve on train vs test data\n- E.g. for different max_depth-s of decision tree\n- Measure accuracy by TimeSeriesSplit"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_tscv_curve(clf, title, X, y):\n    \"\"\"\n    Plots the accuracies on training and testing set for given classifier using Time Series Cross Validation\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.ylim(0.00, 1.05)\n    \n    # Apply Time Series CV to given data\n    train_sizes_manual, train_scores_manual, test_scores_manual = apply_tscv(clf, X, y)\n    \n    # Plot data points for training and testing\n    plt.plot(train_sizes_manual, train_scores_manual, 'o-', color=\"r\", label=\"Training\")\n    plt.plot(train_sizes_manual, test_scores_manual, 'o-', color=\"g\", label=\"Test\")\n    mean_test_acc = [np.mean(test_scores_manual)]*len(test_scores_manual)\n    plt.plot(train_sizes_manual, mean_test_acc, '--', color=\"g\", label=\"Mean\")\n    plt.text(0.5, 0.01, \"Mean=\" + str(round(np.mean(test_scores_manual), 3)), size=\"15\", weight=\"bold\", \n             horizontalalignment=\"center\", verticalalignment=\"bottom\", transform=ax.transAxes)\n    \n    # Labeling\n    plt.xlabel(\"Anzahl Trainingsinstanzen\")\n    plt.ylabel(\"Treffergenauigkeit\")\n    plt.title(title)\n    plt.grid()\n    plt.legend(loc=4)\n    if SAVE_FIG is True:\n        plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Plot-\" + title.replace(\" \", \"\") + \".jpeg\")\n    \n    # Return results for plotting results in master diagram with other clf's results\n    return train_sizes_manual, train_scores_manual, test_scores_manual",
      "execution_count": 141,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## ?.? | Calculate confusion metrics and, if applicable, plot confusion matrix"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def calculate_confusion_metrics(y_actual, y_predicted):\n    \"\"\"\n    Calculates precision, recall and f-measure and, if applicable, plots confusion matrix\n    \"\"\"\n    # 1. Calculate metrics\n    precision = metrics.precision_score(y_actual, y_predicted) if (1 in y_predicted and 1 in y_actual) else -1\n    recall = metrics.recall_score(y_actual, y_predicted) if (1 in y_predicted and 1 in y_actual) else -1\n    f_measure = metrics.f1_score(y_actual, y_predicted) if (precision > 0 or recall >0) else -1\n    if VERBOSE is True:\n        print(\"calculate_confusion_metrics() -> Result on stock=\" + str(stock_ticker) + \" is precision=\" \n              + str(round(precision, 3)) + \", recall=\" + str(round(recall, 3)) + \", f_measure=\" + str(round(f_measure, 3)))\n    \n    # 2. Plot confusion matrix via Seaborn if applicable\n    if SAVE_FIG is True:\n        confusion_matrix = metrics.confusion_matrix(y_actual, y_predicted)\n        fig, ax = plt.subplots()\n        sns_ax = sns.heatmap(confusion_matrix, annot=True, annot_kws={'size':15}, fmt='g', cmap=plt.cm.Blues, cbar=False, square=True)\n        sns_ax.invert_yaxis()\n        sns_ax.invert_xaxis()\n        ax.set(title= str(g_classifier) + \", \" + str(g_stock_ticker) + \", Horizont-Index=\" + str(g_horizon_index), ylabel=\"Korrekte Klasse\", xlabel=\"Vorhergesagte Klasse\")\n        plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/ConfusionMatrix-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \"-\" + str(g_stock_ticker) + \".jpeg\")\n        plt.close()\n    \n    return precision, recall, f_measure",
      "execution_count": 142,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 6 | Final Evaluations\n- In the following, above functions will be called on different datasets in order to evaluate classifiers (Dummy, DT, RF) in different settings\n \n## 6.0 | Evaluation settings\n- For flexible switching of settings (model, toggle feature extraction & tuning, etc.), the following cell serves as the single point of truth that all below functions are based on"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "evaluation_model = DecisionTreeClassifier() # DummyClassifier() vs DecisionTreeClassifier() vs RandomForestClassifier()\nevaluation_horizons_indices = list(range(0, len(TIME_HORIZONS))) # Which time horizon INDICES should be applied? E.g. list(range(0, len(TIME_HORIZONS))) for all horizon INDICES\nuse_feature_extraction = False # Extract features with technical analysis? True vs False\nuse_tuning = False # Tune hyperparameters of evaluation_model? True vs False\n\n# Define verbosity\nVERBOSE = False\n\n# Centrally define if figures should be saved to ./plots/\nSAVE_FIG = False",
      "execution_count": 146,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Introduce global variables for easy plotting and easy averages across models/horizons/etc."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# These global strings should be set be the user. Will be used in .savefig() calls\ng_classifier = \"TBD\" # This string will be included in the plot filenames, e.g. \"Tuned RF\", \"RF\", \"Dummy Classifier\", etc.\ng_plot_folder = \"Feature-Importances-TBD\" # Which subfolder of ./Plots is most suitable for planned plots? e.g. \"Feature-Importances-DT\"\n\n# These global variables are set programatically and do NOT need to be set in advance by the user. Useful for global calculations (e.g. averages)\ng_horizon_index = None # Will be included in the plot filenames, e.g. 2\ng_stock_ticker = None # Will be included in the plot filenames, e.g. \"AAPL\"\ng_feature_names = None # Useful for accessing feature names at any point at any time, e.g. when calculating feature importances\ng_current_tscv_epoch = None # Will be included in the plot filenames, e.g. 3\ng_feature_importances = [] # Useful for averaging feature importances independ of loop structures, e.g. average per horizon across all epochs",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.1 | Get confusion metrics via TSCV\n- Apply time series cross validation (tscv) to calculate confusion metrics for a given classifier on given stock for each time horizon"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_tscv_confusion_meterics(clf):\n    \"\"\"\n    Calculates confusion metrics for each stock (n=11) and for each horizon (n=6) for a total of 66 combinations plus TECH GROUP average\n    \"\"\"\n    # 0. Store interim results for later averaging across datasets\n    y_act_TechGroup = [[] for h in range(0, len(TIME_HORIZONS))]\n    y_pred_TechGroup = [[] for h in range(0, len(TIME_HORIZONS))]\n    stock_ticker = None\n    \n    # 1. Load and loop over stock datasets\n    for stock in [AAPL_DATA]:# TECH_GROUP:\n        print(\"get_tscv_results() -> Stock=\" + str(stock))\n        \n        # 2. Loop over prediction horizons\n        for horizon in evaluation_horizons_indices:\n            global g_horizon_index # make current horizon index accessible globally for .savefig()s\n            g_horizon_index = str(horizon)\n            \n            # 3. Load data for current stock-horizon combination. Train_size=100% because train-test splits will be executed by TSCV function\n            stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename=stock, extract_features=use_feature_extraction, horizon_index=horizon, train_size=1)\n            if horizon == 0: # make current stock ticker index accessible globally for .savefig()s\n                global g_stock_ticker \n                g_stock_ticker = str(stock_ticker)\n            \n            # 4. Get all TSCV predictions for confusion matrix and remember them for later averaging across all stocks\n            y_act, y_pred = apply_tscv(clf, X_train, y_train, return_predictions=True, tuning=use_tuning, importances_only=False)\n            y_act_TechGroup[horizon].extend(y_act)\n            y_pred_TechGroup[horizon].extend(y_pred)\n\n            # 5. Create confusion matrices and save figures if applicable\n            precision, recall, f_measure = calculate_confusion_metrics(y_act, y_pred)\n        \n    print(\"get_tscv_results() -> Stock=Tech Group Average\")\n    g_stock_ticker = str(\"Tech-Group-AVG\")\n    for horizon in  range(0, len(TIME_HORIZONS)):\n        precision, recall, f_measure = calculate_confusion_metrics(y_act_TechGroup[horizon], y_pred_TechGroup[horizon])\n    \n    return",
      "execution_count": 149,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "get_tscv_confusion_meterics(evaluation_model)",
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": "get_tscv_results() -> Stock=Datasets/Kaggle_SnP500_AAPL_2013-2018.csv\nget_tscv_results() -> Stock=Tech Group Average\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.2 | Get feature importances via TSCV\n- Apply time series cross validation (tscv) to calculate feature importances for a given classifier on given stock for each time horizon\n- Only works for decision tree and random forest, but not for dummy classifier"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_tscv_feature_importances(clf):\n    \"\"\"\n    Calculates feature importances of given classifier for each epoch (e.g. n=10) and for each horizon (e.g. n=6) for a total of 66 combinations\n    \"\"\"\n    # 1. Load and loop over stock datasets\n    for stock in [AAPL_DATA]:# TECH_GROUP:\n        print(\"get_tscv_feature_importances() -> Stock=\" + str(stock))\n        \n        # 2. Loop over prediction horizons\n        for horizon in evaluation_horizons_indices:\n            global g_horizon_index # make current horizon index accessible globally for .savefig()s\n            g_horizon_index = str(horizon)\n            \n            # 3. Load data for stock-horizon combination. Train_size=100% because train-test splits will be executed by TSCV function\n            stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename=stock, extract_features=use_feature_extraction, horizon_index=horizon, train_size=1)\n            if horizon == 0:\n                global g_stock_ticker # make current stock ticker index accessible globally for .savefig()s\n                g_stock_ticker = str(stock_ticker)\n            \n            # 4. Calculate feature importances\n            # Feature importances for each TSCV epoch within apply_tscv() are stored in the global variable g_feature_importances\n            apply_tscv(clf, X_train, y_train, return_predictions=return_predictions, tuning=use_tuning, importances_only=True)\n            # Here we calculate the average feature importances for current horizon (across all epochs). The importances for horizon-epoch combinations are covered in above .apply_tscv()\n            global g_feature_importances\n            # For every single feature, sum up the importances from all epochs. Afterwards, divivde by number of epochs to get the average importance of that feature\n            horizon_imps_sum = [0 for epoch in range(0, len(g_feature_importances[0]))]\n            for imps in g_feature_importances:\n                for i in range(0, len(imps)):\n                    horizon_imps_sum[i] = horizon_imps_sum[i] + imps[i]\n            horizon_imps_avg = [x/len(g_feature_importances) for x in horizon_imps_sum]\n\n            # 5. Plot horizontal bars of the average feature importances in current horizon (Top10)\n            feature_ranking_indicies = np.argsort(horizon_imps_avg)\n            top10_features_indices = feature_ranking_indicies[-10:]\n            fig = plt.figure()\n            plt.title(\"Einflussgrade der Features, \" + str(g_classifier) + \", AAPL\")\n            plt.barh(y=[g_feature_names[i] for i in top10_features_indices], width=[horizon_imps_avg[i] for i in top10_features_indices])\n            fig.tight_layout()\n            if SAVE_FIG is True:\n                plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Top10Importances-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \n                            \"-EpochsAVG-\" + str(g_stock_ticker) + \".jpeg\")\n            plt.close()\n            # Reset feature importances for next horizon\n            g_feature_importances = []\n    \n    return",
      "execution_count": 96,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "get_tscv_feature_importances(evaluation_model)",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": "get_tscv_feature_importances() -> Stock=Datasets/Kaggle_SnP500_AAPL_2013-2018.csv\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'return_predictions' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-9af528600c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_tscv_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_FIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-a9c316c52806>\u001b[0m in \u001b[0;36mget_tscv_feature_importances\u001b[0;34m(clf, verbose, save_fig)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# 4. Calculate feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Feature importances for each TSCV epoch within apply_tscv() are stored in the global variable g_feature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mapply_tscv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_predictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Here we calculate the average feature importances for current horizon (across all epochs). The importances for horizon-epoch combinations are covered in above .apply_tscv()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mg_feature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'return_predictions' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## OLD: 6.? | Get TSCV results (confusion metrics or feature importances) per classifier and per stock\n- Apply time series cross validation (tscv) to calculate either confusion metrics or feature importances for a given classifier and stock"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_tscv_results(clf, tuning=use_tuning, return_predictions=False, importances_only=False):\n    \"\"\"\n    Calculates either confusion matrice for each stock (n=11) and for each horizon (n=6) for a total of 66 combinations,\n    or calculates feature importances only (if importances_only set to True)\n    \"\"\"\n    # 0. Store results for averaging across datasets\n    y_act_TechGroup = [[] for h in range(0, len(TIME_HORIZONS))]\n    y_pred_TechGroup = [[] for h in range(0, len(TIME_HORIZONS))]\n    stock_ticker = None\n    \n    # 1. Load and loop over stock datasets\n    for stock in [AAPL_DATA]:# TECH_GROUP:\n        print(\"get_tscv_results() -> Stock=\" + str(stock))\n        # 2. Loop over prediction horizons: 1, 5, 10, 20, 65, 250 trading (!) days\n        for horizon in  range(0, len(TIME_HORIZONS)):\n            global g_horizon_index # make current horizon index accessible globally for .savefig()s\n            g_horizon_index = str(horizon)\n            # 3. Load data for stock-horizon combination. Train_size=100% because train-test splits will be done by TSCV function\n            stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename=stock, extract_features=True, horizon_index=horizon, train_size=1)\n            if horizon == 0:\n                global g_stock_ticker # make current stock ticker index accessible globally for .savefig()s\n                g_stock_ticker = str(stock_ticker)\n            \n            # Case 1: Invoker of funcion is interested in feature importances only (no confusion metrics)\n            if importances_only is True:\n                # Get feature importances only, no actual/predicted y-s and no confusion metrics\n                apply_tscv(clf, X_train, y_train, return_predictions=return_predictions, tuning=tuning, importances_only=True)\n                \n                # Here we calculate the average feature importances for current horizon (across all epochs). The importances for horizon-epoch combinations are covered in above .apply_tscv()\n                global g_feature_importances\n                # For every single feature, sum up the importances from all epochs. Afterwards, divivde by number of epochs to get the average importance of that feature\n                horizon_imps_sum = [0 for epoch in range(0, len(g_feature_importances[0]))]\n                for imps in g_feature_importances:\n                    for i in range(0, len(imps)):\n                        horizon_imps_sum[i] = horizon_imps_sum[i] + imps[i]\n                horizon_imps_avg = [x/len(g_feature_importances) for x in horizon_imps_sum]\n                \n                # Now plot horizontal bars of the average feature importances in current horizon (Top10)\n                feature_ranking_indicies = np.argsort(horizon_imps_avg)\n                top10_features_indices = feature_ranking_indicies[-10:]\n                # Plot feature importances in horizontal bars\n                fig = plt.figure()\n                plt.title(\"Einflussgrade der Features, \" + str(g_classifier) + \", AAPL\")\n                plt.barh(y=[g_feature_names[i] for i in top10_features_indices], width=[horizon_imps_avg[i] for i in top10_features_indices])\n                fig.tight_layout()\n                if SAVE_FIG is True:\n                    plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Top10Importances-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \n                                \"-EpochsAVG-\" + str(g_stock_ticker) + \".jpeg\")\n                plt.close()\n                # Reset for next horizon\n                g_feature_importances = []\n                \n            # Case 2: Invoker of funcion is interested in confusion metrics (and not feature importances)\n            else:\n                # 2b. Get TSCV predictions for confusion matrix and remember for later averaging across all stocks\n                y_act, y_pred = apply_tscv(clf, X_train, y_train, return_predictions=return_predictions, tuning=tuning, importances_only=False)\n                y_act_TechGroup[horizon].extend(y_act)\n                y_pred_TechGroup[horizon].extend(y_pred)\n                \n                # 3. Create confusion matrices and save figures if applicable\n                precision, recall, f_measure = calculate_confusion_metrics(stock_ticker, horizon, y_act, y_pred)\n        \n    # 4. If metrics sought, average for each horizon across datasets\n    if importances_only is not True:\n        print(\"get_tscv_results() -> Stock=Tech Group Average\")\n        for horizon in  range(0, len(TIME_HORIZONS)):\n            precision, recall, f_measure = calculate_confusion_metrics(\"Tech-Group-AVG\", horizon, y_act_TechGroup[horizon], y_pred_TechGroup[horizon])",
      "execution_count": 117,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Throw Dummy, DT, RF into get_tscv_results()\n# dummy = DummyClassifier(random_state=RANDOM_SEED)\n# dt = DecisionTreeClassifier(random_state=RANDOM_SEED)\n# rf = RandomForestClassifier(n_estimators=5, random_state=RANDOM_SEED)\n\n# get_tscv_results(rf, tuning=False, return_predictions=False, importances_only=True)",
      "execution_count": 113,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.TBD | Decision tree hyperparameter variation example (max_depth)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_dt(filename=AAPL_DATA, extract_features=True, horizon_index=2):\n    # Load data\n    stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename, extract_features, horizon_index, train_size=1)\n\n    # Generate classifier\n    clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n\n    # Gather all plotted series to create master plot\n    plotted_Xs = []\n    plotted_ys_train = []\n    plotted_ys_test = []\n\n    # Evaluate classifier for different values of selected hyperparameter\n    for i in [1, 3, 5, 6, 7, 9, 11, None]:\n        clf.set_params(max_depth=i)\n        X_res, y_train_res, y_test_res = plot_tscv_curve(clf, \"Decision Tree Treffergenauigkeiten AAPL (MAX_DEPTH=\" + str(i) + \")\", X_train, y_train)\n        print(\"Mean test accuracy for MAX_DEPTH=\" + str(i) + \": \" + str(round(np.mean(y_test_res), 3)))\n        \n        # Throw best params from 2.2 (Hyperparameter tuning) in and see if same result\n        if i == 7:\n            print(\"i is 7, so here we try the GRID_SEARCHED-best params: 'max_depth': 29, 'max_features': 29, 'min_samples_leaf': 1, 'min_samples_split': 15}, achieved 58.3% acc above\")\n            clf_best_params = DecisionTreeClassifier(random_state=RANDOM_SEED, max_depth=29, max_features=29, min_samples_leaf=1, min_samples_split=15)\n            X_res, y_train_res, y_test_res = plot_tscv_curve(clf_best_params, \"Decision Tree Treffergenauigkeiten AAPL (BEST_PARAMS)\", X_train, y_train)\n            print(\"Best-params DT achievend ACC=\" + str(i) + \": \" + str(round(np.mean(y_test_res), 3)))\n        \n        # TBD: Use all results to make master plot\n        plotted_Xs.extend([X_res])\n        plotted_ys_train.extend([y_train_res])\n        plotted_ys_test.extend([y_test_res])",
      "execution_count": 115,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# evaluate_dt(filename=AAPL_DATA, extract_features=True, horizon_index=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.TBD | Random forest hyperparameter variation example (n_estimators)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_rf(filename=AAPL_DATA, extract_features=True, horizon_index=2):\n    # Load data\n    stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename, extract_features, horizon_index, train_size=1)\n\n    # Generate classifier\n    clf = RandomForestClassifier(n_estimators = 1, random_state=RANDOM_SEED)\n\n    # Evaluate classifier for different values of selected hyperparameter\n    for i in [1, 5, 10, 50, 100]:\n        clf.set_params(n_estimators=i)\n        X_res, y_train_res, y_test_res = plot_tscv_curve(clf, \"Random Forest Treffergenauigkeiten AAPL (N_ESTIMATORS=\" + str(i) + \")\", X_train, y_train)\n        print(\"Mean test accuracy for N_ESTIMATORS=\" + str(i) + \": \" + str(round(np.mean(y_test_res), 3)))\n    \n    # TBD: Plotting",
      "execution_count": 116,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# evaluate_rf(filename=MSFT_DATA, extract_features=True, horizon_index=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# --------------------------------------------Ignore below------------------------------------------------"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.3 | Test master function\n- Varying 4-5 dimensions simultaneously too complex, hence all but classifier is fixed in the master function\n- TBD: Toggle Hyperparameter Tuning somewhere\n- TBD: Correct testing for 0 values at the end, e.g. 365 horizon return in last week's data is always 0!! Must be accounted for"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TIME_HORIZONS = [1, 5, 10, 20, 65, 250] <- use index from 0 (1 trading day) to 5 (250 trading days) to determine time horizon\ndef master_function(time_horizon=2):\n    # Create classifiers that are to be evaluated\n    clf1 = DummyClassifier(random_state=42)\n    clf2 = DecisionTreeClassifier(random_state=42)\n    clf3 = RandomForestClassifier(n_estimators=10, random_state=42)\n    clfs = [clf1, clf2, clf3]\n    \n    # Iterate over models & datasets to evaluate classifiers on each dataset\n    for clf in clfs:\n        cum_acc = 0\n        for dataset_path in TECH_GROUP:\n            # Load data and evaluate all models on it\n            evaluate_models(dataset=dataset_path, extract_features=False, time_horizon=time_horizon)\n            cum_acc = cum_acc + evaluate_models(dataset=dataset_path, extract_features=True, time_horizon=time_horizon)\n        clf_avg_acc = cum_acc/len(TECH_GROUP)\n        print(\"avg acc for \" + str(type(clf)) + \" is \" + str(round(clf_avg_acc, 2)))",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# master_function()",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.1.2 | Classifier horizon variation (averaged aross TECH_GROUP)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def eval_avg_across_tech(clf):\n    \"\"\"\n    Evaluates accuracy of given classifier for each time horizon (accuracy average cross TECH_GROUP)\n    \"\"\"\n    accs_horizons = []\n    for horizon in range(0, 6):\n        accs_datasets = []\n        for file in TECH_GROUP:\n            # 1. Load data\n            stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(file, horizon_index=horizon, train_size=1)\n            \n            # 2. Evaluate classifier\n            train_sizes_manual, train_scores_manual, test_scores_manual = apply_tscv(clf, X_train, y_train)\n#             print(\"ACC for horizon=\" + str(horizon) + \" & file=\" + str(file) + \" is \" + str(np.mean(test_scores_manual)))\n            accs_datasets.extend([np.mean(test_scores_manual)])\n        print(\"ACC for horizon=\" + str(horizon) + \" is \" + str(np.mean(accs_datasets)))\n        accs_horizons.extend([np.mean(accs_datasets)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n# eval_avg_across_tech(clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# clf = RandomForestClassifier(random_state=RANDOM_SEED)\n# eval_avg_across_tech(clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Comparison table: Model-Extractets-Horizon"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "- DT auf AAPL: Bei Variierung von MAX_DEPTH ist MAX(EXTRACTEDS)=56.2 , MAX(NO EXTRACTEDS)=53.3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}