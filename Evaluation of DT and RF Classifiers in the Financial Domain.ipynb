{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# BSc Thesis: Evaluation of Decision Tree and Random Forest Classifiers in the Finance Domain"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Table of Contents\n0. Preparation\n1. Data Preparation Stage\n2. Feature Extraction\n3. Classification\n4. Evaluation of Models\n5. Visualisations"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# 0 | Preparation"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Imports"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Data manipulation and arrays\nimport pandas as pd\nimport numpy as np\n\n# Machine learning\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, GridSearchCV\nfrom sklearn import metrics\n\n# Plottig\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom IPython.display import display",
      "execution_count": 294,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Constant variables"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define filenames for technology stock .CSV datasets\nMSFT_DATA = \"Datasets/Kaggle_SnP500_MSFT_2013-2018.csv\"\nAAPL_DATA = \"Datasets/Kaggle_SnP500_AAPL_2013-2018.csv\"\nGOOGL_DATA = \"Datasets/Kaggle_SnP500_GOOGL_2013-2018.csv\"\nHP_DATA = \"Datasets/Kaggle_SnP500_HP_2013-2018.csv\"\nIBM_DATA = \"Datasets/Kaggle_SnP500_IBM_2013-2018.csv\"\nWU_DATA = \"Datasets/Kaggle_SnP500_WU_2013-2018.csv\"\nXRX_DATA = \"Datasets/Kaggle_SnP500_XRX_2013-2018.csv\"\nTECH_GROUP = [MSFT_DATA, AAPL_DATA, GOOGL_DATA, HP_DATA, IBM_DATA, WU_DATA, XRX_DATA] # this list will be used to iterate over all datasets\n\n# Define time horizons to compare classification results for 1-day to 1-year predictions\nTIME_HORIZONS = [1, 5, 10, 30, 100, 365]\n\n# Define verbosity\nVERBOSE = False\n\n# Centrally define if figures should be saved to ./plots/\nSAVE_FIG = False",
      "execution_count": 335,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1 | Data Preparation Stage\n- Load data and adjust columns as needed\n- Extract features for technical analysis\n- Define class for later classification\n- Detect anomalies in the datasets\n- No feature selection needed as embedded in Decision Trees (DT) and Random Forests (RF)\n\n## 1.1 | Load Datasets\n- For an apples-to-apples comparison, technology companies are analyzed (idea: companies/stocks within an industry have similar drivers)\n- Selected stocks differ in price trends (upward- vs constant- vs downward trend)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_OHLC_data(filename=MSFT_DATA, time_horizons=TIME_HORIZONS, verbose=VERBOSE, save_fig=SAVE_FIG):\n    \"\"\"\n    Loads basic stock data (date, name, open, high, low, close) from a given .CSV file and returns a corresponding DataFrame.\n    Unnecessary categorical columns are dropped, and necessary columns (e.g. month as number) are added.\n    \"\"\"\n    try:\n        df = pd.read_csv(filename)\n        \n        if save_fig is True:\n            # Visualize loaded time series data\n            df[\"date\"] = pd.to_datetime(df[\"date\"])\n            df.plot(x=\"date\", y=\"close\", figsize=(12,6), legend=None)\n            plt.xlabel(\"Time [Year]\")\n            plt.ylabel(\"Price [daily closing price in USD]\")\n            plt.title(df[\"Name\"][0] + \"-Stock Data 2013 to 2018\");\n            plt.savefig(\"./Plots/\" + df[\"Name\"][0] + \"-Stock-Price-Plot.jpeg\")\n        \n        # Calculate base column for later class: future return of stock over given time horizon (e.g. this week's Monday to next week's Monday)\n        for horizon in time_horizons:\n            df[\"return_future_\" + str(horizon) + \"d\"] = (df[\"close\"].shift(-1*horizon)/df[\"close\"])-1\n        \n        # Convert date to numerical month to possibly detect cyclicality (e.g. christmas effect) in time series\n        df[\"month\"] = df[\"date\"].astype(\"datetime64[ns]\").dt.month\n        \n        if verbose is True:\n            print(\"Loaded DataFrame has the following columns:\")\n            for col in df:\n                print(\"Column \\'\" + col + \"\\' with type\", type(df[col][0]), \", e.g.\", df[col][0])\n            print(\"df.head():\")\n            print(df.head())\n        \n        return df\n    except:\n        print(\"Error, failed to find or load OHLC data from file with name \\'\"\n              + filename + \"\\'. Please provide well-formed CSV file with OHLC stock data\")",
      "execution_count": 352,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_MSFT = load_OHLC_data(MSFT_DATA)\n# df_AAPL = load_OHLC_data(AAPL_DATA)\n# df_GOOGL = load_OHLC_data(GOOGL_DATA)\n# df_HP = load_OHLC_data(HP_DATA)\n# df_IBM = load_OHLC_data(IBM_DATA)\n# df_WU = load_OHLC_data(WU_DATA)\n# df_XRX = load_OHLC_data(XRX_DATA)",
      "execution_count": 353,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.2 | Extract Features\n- Common metrics for technical analysis are calculated to be later used as features\n- Source for technical indicators: TA-lib, https://www.quantopian.com/posts/technical-analysis-indicators-without-talib-code"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def extract_OHLC_features(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Extract common technical stock analysis features from given OHLC stock data for distinct time horizons\n    \"\"\"\n    # Calculate technical features for each time horizon\n    for horizon in time_horizons:\n#         # Future return of stock over given time horizon (e.g. this week's Monday to next week's Monday)\n#         df[\"return_future_\" + str(horizon) + \"d\"] = (df[\"close\"].shift(-1*horizon)/df[\"close\"])-1\n        \n        # Past return of stock over given time horizon (e.g. last week's Monday to this week's Monday)\n        df[\"return_past_\" + str(horizon) + \"d\"] = (df[\"close\"].shift(horizon)/df[\"close\"])-1\n        \n        # Implied volatility measured by standard deviation\n        df[\"volatility_\" + str(horizon) + \"d\"] = df[\"close\"].rolling(horizon).std()\n        \n        # Moving averages (ma)\n        df[\"ma_\" + str(horizon) + \"d\"] = df[\"close\"].rolling(horizon).mean()\n        \n#         Exponentially-weighted moving average (ewma)\n#         df[\"ewma_\" + str(horizon) + \"d\"] = pd.ewma(df[\"close\"], span=horizon, min_periods=horizon-1)\n#         df[\"ewma_\" + str(horizon) + \"d\"] = df[\"close\"].ewm(span=horizon, min_periods=horizon-1)\n        \n        # Momentum (absolute change in price over past horizon)\n        df[\"momentum_\" + str(horizon) + \"d\"] = df[\"close\"].diff(horizon)\n        \n        # Rate of change during horizon period\n        df[\"rateofchange_\" + str(horizon) + \"d\"] = (df[\"close\"].diff(horizon-1)) / (df[\"close\"].shift(horizon-1))\n        \n#         Bollinger Bands\n#         df[\"bollingerbands1_\" + str(horizon) + \"d\"] = 4*df[\"volatility_\" + str(horizon) + \"d\"] / df[\"ma_\" + str(horizon) + \"d\"]\n#         df[\"bollingerbands2_\" + str(horizon) + \"d\"] = (df[\"close\"] - df[\"ma_\" + str(horizon) + \"d\"] + 2*df[\"volatility_\" + str(horizon) + \"d\"]) / 4*df[\"volatility_\" + str(horizon) + \"d\"]\n        \n        # TBD add other talib indicators from #Pivot Points, Supports and Resistances\n    \n    # OHLC average is used for stock price average of a given day\n    df[\"ohlc_avg\"] = df[[\"open\", \"high\", \"low\", \"close\"]].mean(axis=1)\n    \n    # Replace NaNs with zeroes\n    df = df.fillna(value=0)\n    return df\n\n\nprint(\"#Features before extraction:\", len(df_MSFT.columns))\ndf_MSFT = extract_OHLC_features(df_MSFT, TIME_HORIZONS)\nprint(\"#Features after extraction:\", len(df_MSFT.columns))\ndf_AAPL = extract_OHLC_features(df_AAPL, TIME_HORIZONS)\ndf_GOOGL = extract_OHLC_features(df_GOOGL, TIME_HORIZONS)\ndf_HP = extract_OHLC_features(df_HP, TIME_HORIZONS)\ndf_IBM = extract_OHLC_features(df_IBM, TIME_HORIZONS)\ndf_WU = extract_OHLC_features(df_WU, TIME_HORIZONS)\ndf_XRX = extract_OHLC_features(df_XRX, TIME_HORIZONS)\n\nif VERBOSE is True:\n    print(\"With extracted features, dfMSFT.head() now yields following format:\")\n    print(df_MSFT.head())",
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": "#Features before extraction: 45\n#Features after extraction: 45\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## 1.3 | Anomaly Detection\n- Anomaly defined as: ABS(return_past_1d) > threshold=5% (default)\n- Such anomalies (5% threshold) occur in about 1.4% of instances for seven tech stock datasets"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "def detect_anomalies(df, verbose=VERBOSE, threshold=0.05):\n    \"\"\"\n    Iterates through the DataFrame and prints out all dates where 1-day-return is greater than threshold=5% (default)\n    \"\"\"\n    if verbose is True:\n        print(\"Detecting anomalies where abs(1-day-return)>\" + str(threshold*100) + \" % for \" + df[\"Name\"][0])\n    for i in range(len(df)):\n        x = df[\"return_past_1d\"][i]\n        d = df[\"date\"][i]\n        if (abs(x) > threshold):\n            global anomaly_counter\n            anomaly_counter = anomaly_counter + 1\n            if verbose is True:\n                print(\"Anomaly: 1-day-return of \" + str(round(x * 100, 2)) + \"% on \" + d.strftime(\"%A, %d.%m.%Y\"))\n\n\nanomaly_counter = 0\ndetect_anomalies(df_MSFT)\ndetect_anomalies(df_AAPL)\ndetect_anomalies(df_GOOGL)\ndetect_anomalies(df_HP)\ndetect_anomalies(df_IBM)\ndetect_anomalies(df_WU)\ndetect_anomalies(df_XRX)\n\nprint(\"anomaly_counter=\" + str(counter) + \", or \" + str(round(counter*100/(7*len(df_MSFT)), 2)) + \"% of instances\")",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": "anomaly_counter=124, or 1.41% of instances\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.4 | Define classes\n- This notebook evaluates DT and RF for stock recommendation (application no. 2 in thesis)\n- Classes are defined for each time horizon to enable for later comparisons\n- Base columns, on which classes are built, are removed to prevent illegal future-peeking features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def define_classes(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Create target column in df: 1 means 'Yes, investor should buy stock', 0 means 'No, investor should not buy stock'.\n    The assumed trading strategy here is, that the investor buy the stock on a given date and sells it after the horizon period.\n    Also removes illegal (future-peeking) columns\n    \"\"\"\n    for horizon in time_horizons:\n        base_column_name = \"return_future_\" + str(horizon) + \"d\"\n        class_name = \"class_\" + str(horizon) + \"d\"\n        \n        if class_name not in df.columns:\n            df[class_name] = np.where(df[base_column_name] > 0, 1, 0)\n            # Remove base column as it would be an illegal (future-peeking) feature\n        if base_column_name in df.columns:\n            df = df.drop(columns=[base_column_name])\n    return df\n\n\ndf_MSFT = define_classes(df_MSFT)\ndf_AAPL = define_classes(df_AAPL)\ndf_GOOGL = define_classes(df_GOOGL)\ndf_HP = define_classes(df_HP)\ndf_IBM = define_classes(df_IBM)\ndf_WU = define_classes(df_WU)\ndf_XRX = define_classes(df_XRX)\n\nprint(\"classes (class_<horizon>d) created, base columns (return_future_<horizon>d) removed\")",
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": "classes (class_<horizon>d) created, base columns (return_future_<horizon>d) removed\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.5 | Check class balance\n- The two classes Yes (1) and No (0) should be balanced, else the evaluation technique must be adapted"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def check_class_balance(df=df_MSFT, time_horizons=TIME_HORIZONS, verbose=VERBOSE, save_fig=SAVE_FIG):\n    for horizon in time_horizons:\n        class_name = \"class_\" + str(horizon) + \"d\"\n        if verbose is True:\n            print(df[class_name].value_counts())\n\n        fig = plt.figure()\n        df[class_name].hist()\n        plt.xlabel(\"Class Value\")\n        plt.ylabel(\"Frequency\")\n        plt.title(df[\"Name\"][0] + \"-Class Value Histogram-\" + str(horizon) + \"d\")\n        if save_fig is True:\n            plt.savefig(\"./Plots/Class-Balance-Check/\" + df[\"Name\"][0] + \"-Class-Balance-Histogram-\" + str(horizon) + \"d.jpeg\")\n#         plt.close(fig) # clean memory if needed\n\n\ncheck_class_balance(df_MSFT)\ncheck_class_balance(df_AAPL)\ncheck_class_balance(df_GOOGL)\ncheck_class_balance(df_HP)\ncheck_class_balance(df_IBM)\ncheck_class_balance(df_WU)\ncheck_class_balance(df_XRX)",
      "execution_count": 158,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##  TBD: 1.6 | Seaborn feature correlation plots\n\n- Results: TBD\n- Test old: Ergebnis: volatility-volume stark pos. korreliert (0.45), ohlc_avg-volume mäßig neg. korreliert (-0.36)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# def plot_corr_sns(df):\n#     df = df.drop(columns=[\"open\", \"high\", \"low\", \"close\", \"Name\"])\n#     corr = df.corr()\n    \n#     plt.figure()\n#     f, ax = plt.subplots(figsize=(5, 4)) #PARAM: figsize=(15, 12)\n#     ax.set_title(\"Feature Correlation Matrix\")\n#     sns.heatmap(corr, cmap=plt.cm.Blues, mask=np.zeros_like(corr, dtype=np.bool), square=True, ax=ax)\n# #     # plt.savefig(\"./plots/DataPreparation_MSFT-Stock-Data_correlation-matrix_v4.jpeg\")\n    \n# #     plt.figure()\n# #     sns.relplot(x=\"volume\", y=\"volatility_3d\", data=df);\n    \n# #     plt.figure()\n# #     sns.relplot(x=\"volume\", y=\"daily_return\", data=df);\n    \n# #     plt.figure()\n# #     sns.relplot(x=\"ohlc_avg\", y=\"ma_3\", data=df);\n\n\n# plot_corr_sns(df_MSFT)\n# # Add other df's",
      "execution_count": 164,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# 2 | Classification Stage\n- Apply classifiers on datasets (for each time horizon-company combination)\n- Plot findings"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.1 | Define training and test sets\n- Seed RandomState to make algorithms reproducible"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train_test_split_data(df, train_size=0.7, time_horizons=TIME_HORIZONS, verbose=VERBOSE):\n    \"\"\"\n    Generates training and testing set from a given DataFrame dataset.\n    Assumes last COUNT(time_horizons) column(s) in DataFrame are classes,\n    others are features.\n    Returns features (X) and targets (y) in training- and testing sets.\n    \"\"\"\n    # Remove non-numerical features for .fit() to work\n    df = df.drop(columns=[\"Name\", \"date\"])\n    \n    # Split DataFrame into features (X) and target (y)\n    X = df.iloc[:,:-1*len(time_horizons)]\n    y = df.iloc[:,-1*len(time_horizons):]\n    \n    # Use first (in same chronological order as time series) 70% to train and last 30% to test\n    split_index = int(len(X) * train_size)\n    if verbose is True:\n        print(\"Split ist bei Index \" + str(split_index) + \" von \" + str(len(X)))\n    \n    X_train, X_test = X[:split_index], X[split_index:]\n    y_train, y_test = y[:split_index], y[split_index:]\n    return X_train, X_test, y_train, y_test",
      "execution_count": 369,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_MSFT_train, X_MSFT_test, y_MSFT_train, y_MSFT_test = train_test_split_data(df_MSFT)\n# Add other df's\n# Better idea: just call this function later in evaluation, no extra variables needed",
      "execution_count": 354,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 2.2 Hyperparameter tuning"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3 | Evaluation Stage\n- Build confusion matrixes and calculate performance metrics \n- Plot findings\n\n## 3.1 | Measure accuracy by TimeSeriesSplit\n- Data is in time series format, hence special time sseries cross validation method is more adequate than traditional k-fold cross validation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TBD: from sklearn.model_selection import TimeSeriesSplit",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.2 | Measure accuracy for given classifier and given X-s & y-s"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_accuracy(clf, X_train, X_test, y_train, y_test, verbose=VERBOSE):\n    \"\"\"\n    Evaluates accuracy of given classifier for given training and test data.\n    Only accepts y-DataFrames with one single column (as only one class is predicted)\n    \"\"\"\n    if not isinstance(y_train, pd.Series) or not isinstance(y_test, pd.Series):\n        print(\"y-DataFrames must have 1 column only.\")\n        return -1\n    \n    clf.fit(X_train, y_train)\n    acc = clf.score(X_test, y_test)\n    if verbose is True:\n        print(\"model=\" + type(clf) + \", class=\" + y_train.name + \", acc=\" + str(round(acc, 2)))\n    return acc",
      "execution_count": 368,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.3 | Build confusion matrixes"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TBD: Confusion Matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 4 | Putting it all together (all above methods are called here in one single place)\n- Prepare, classify and evaluate datasets"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.1 | Summarizing function for loading & extracting vs not data & splitting for train vs test"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_train_test_data(filename=MSFT_DATA, extract_features=False):\n    \"\"\"\n    Serves as a one-stop-shop function for data loading incl. preparation so that classifier only relies on this single fucntion (and not many single functions) \n    \"\"\"\n    # Load data from .CSV (1.1)\n    df = load_OHLC_data(filename=filename)\n    \n    # Extract features if applicable (1.2)\n    if extract_features is True:\n        df = extract_OHLC_features(df)\n    \n    # Detect anomalies (1.3) skipped\n    # Define classes (1.4)\n    df = define_classes(df)\n    \n    # Check class balance (1.5) skipped\n    # Split data into training and testing samples\n    X_train, X_test, y_train, y_test = train_test_split_data(df)\n    \n    return X_train, X_test, y_train, y_test",
      "execution_count": 367,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = generate_train_test_data(extract_features=False)\nprint(X_train.columns)\nprint(y_train.columns)\nX_train, X_test, y_train, y_test = generate_train_test_data(extract_features=True)\nprint(X_train.columns)\nprint(y_train.columns)",
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Index(['open', 'high', 'low', 'close', 'volume', 'month'], dtype='object')\nIndex(['class_1d', 'class_5d', 'class_10d', 'class_30d', 'class_100d',\n       'class_365d'],\n      dtype='object')\nIndex(['open', 'high', 'low', 'close', 'volume', 'month', 'return_past_1d',\n       'volatility_1d', 'ma_1d', 'momentum_1d', 'rateofchange_1d',\n       'return_past_5d', 'volatility_5d', 'ma_5d', 'momentum_5d',\n       'rateofchange_5d', 'return_past_10d', 'volatility_10d', 'ma_10d',\n       'momentum_10d', 'rateofchange_10d', 'return_past_30d', 'volatility_30d',\n       'ma_30d', 'momentum_30d', 'rateofchange_30d', 'return_past_100d',\n       'volatility_100d', 'ma_100d', 'momentum_100d', 'rateofchange_100d',\n       'return_past_365d', 'volatility_365d', 'ma_365d', 'momentum_365d',\n       'rateofchange_365d', 'ohlc_avg'],\n      dtype='object')\nIndex(['class_1d', 'class_5d', 'class_10d', 'class_30d', 'class_100d',\n       'class_365d'],\n      dtype='object')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### TBD: Compare INITIAL DF (call fucntion) with EXTRACTED DF (cal lfunction) to see if engineering any good"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.2 | Apply & evaluate different classifiers on time horizons"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_models(dataset=MSFT_DATA, clf=DummyClassifier(random_state=42), extract_features=False, time_horizon=2, verbose=VERBOSE):\n    \"\"\"\n    Evaluates gives  classifier on given data with or without extracted features for given time horizon\n    \"\"\"\n    # Load data in \"ready-to-classify\" form with one-stop-shop function (4.1)\n    X_train, X_test, y_train, y_test = generate_train_test_data(dataset, extract_features)\n    \n    # Evaluate classifier on data\n    acc = evaluate_accuracy(clf, X_train, X_test, y_train[y_train.columns[time_horizon]], y_test[y_test.columns[time_horizon]])\n    if verbose is True:\n        print(\"clf=\" + str(type(clf)), \"dataset=\" + dataset + \" extracteds=\" + str(extract_features) + \", time_horizon=\" + str(time_horizon) + \", acc=\" + str(round(acc, 2)))\n    return acc",
      "execution_count": 390,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "evaluate_models()",
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 391,
          "data": {
            "text/plain": "0.544973544973545"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.3 | Final master function\n- Varying 4-5 dimensions too complex, hence the time horizon is fixed in the master function\n- TBD: Toggle Hyperparameter Tuning somewhere\n- TBD: Correct testing for 0 values at the end, e.g. 365 horizon return in last week's data is always 0!! Must be accounted for"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TIME_HORIZONS = [1, 5, 10, 30, 100, 365] <- use index from 0 (1 day) to 5 (365 days) to determine time horizon\ndef master_function(time_horizon=2):\n    # Create classifiers that are to be evaluated\n    clf1 = DummyClassifier(random_state=42)\n    clf2 = DecisionTreeClassifier(random_state=42)\n    clf3 = RandomForestClassifier(n_estimators=10, random_state=42)\n    clfs = [clf1, clf2, clf3]\n    \n    # Iterate over models & datasets to evaluate classifiers on each dataset\n    for clf in clfs:\n        cum_acc = 0\n        for dataset_path in TECH_GROUP:\n            # Load data and evaluate all models on it\n            evaluate_models(dataset=dataset_path, extract_features=False, time_horizon=time_horizon)\n            cum_acc = cum_acc + evaluate_models(dataset=dataset_path, extract_features=True, time_horizon=time_horizon)\n        clf_avg_acc = cum_acc/len(TECH_GROUP)\n        print(\"avg acc for \" + str(type(clf)) + \" is \" + str(round(clf_avg_acc, 2)))",
      "execution_count": 392,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "master_function()",
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "text": "avg acc for <class 'sklearn.dummy.DummyClassifier'> is 0.52\navg acc for <class 'sklearn.tree.tree.DecisionTreeClassifier'> is 0.52\navg acc for <class 'sklearn.ensemble.forest.RandomForestClassifier'> is 0.52\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Comparison table: Model-Extractets-Horizon"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}