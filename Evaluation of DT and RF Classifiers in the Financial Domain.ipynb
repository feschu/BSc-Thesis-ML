{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# BSc Thesis: Evaluation of Decision Tree and Random Forest Classifiers in the Finance Domain\nThesis aiming to evaluate the application of decision tree and random forest classifiers within the financial domain\n\n## Table of Contents\n0. Preparation\n1. Data Preparation Stage\n2. Hyperparameter Tuning\n3. Evaluation Stage\n4. Visualisation\n5. Final Evaluations\n6. Reproducing Thesis Results"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# 0 | Preparation\n- Import libraries and define constant variables"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Imports"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Data manipulation and arrays\nimport pandas as pd\nimport numpy as np\n\n# Machine learning\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, learning_curve, GridSearchCV, RandomizedSearchCV\nfrom sklearn import metrics\n\n# Plottig\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom IPython.display import display",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Constant variables"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define filenames for technology stock .CSV datasets\nAAPL_DATA = \"Datasets/Kaggle_SnP500_AAPL_2013-2018.csv\"\nAMZN_DATA = \"Datasets/Kaggle_SnP500_AMZN_2013-2018.csv\"\nCSCO_DATA = \"Datasets/Kaggle_SnP500_CSCO_2013-2018.csv\"\nGE_DATA = \"Datasets/Kaggle_SnP500_GE_2013-2018.csv\"\nGOOGL_DATA = \"Datasets/Kaggle_SnP500_GOOGL_2013-2018.csv\"\nHP_DATA = \"Datasets/Kaggle_SnP500_HP_2013-2018.csv\"\nIBM_DATA = \"Datasets/Kaggle_SnP500_IBM_2013-2018.csv\"\nINTC_DATA = \"Datasets/Kaggle_SnP500_INTC_2013-2018.csv\"\nMSFT_DATA = \"Datasets/Kaggle_SnP500_MSFT_2013-2018.csv\"\nWU_DATA = \"Datasets/Kaggle_SnP500_WU_2013-2018.csv\"\nXRX_DATA = \"Datasets/Kaggle_SnP500_XRX_2013-2018.csv\"\nTECH_GROUP = [AAPL_DATA, AMZN_DATA, CSCO_DATA, GE_DATA, GOOGL_DATA, HP_DATA, IBM_DATA, INTC_DATA, MSFT_DATA, WU_DATA, XRX_DATA]\n\n# Define time horizons to compare classification results for 1-day to 1-year predictions (approx. trading days)\nTIME_HORIZONS = [1, 5, 10, 20, 65, 250]\n\n# Make code reproducible by seeding random states\nRANDOM_SEED = 42",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1 | Data Preparation Stage\n- Load data and adjust columns as needed, extract features for technical analysis, define classes, detect anomalies\n- No feature selection needed as embedded in Decision Trees (DT) and Random Forests (RF)\n- These functions will later be invoked in the Final Evaluations stage\n\n## 1.1 | Load Datasets\n- For an apples-to-apples comparison, technology companies are analyzed (idea: companies/stocks within an industry have similar drivers)\n- Selected stocks differ in price trends (upward- vs constant- vs downward trend)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_OHLC_data(filename=MSFT_DATA, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Loads basic stock data (date, name, open, high, low, close) from a given .CSV file and returns a corresponding DataFrame.\n    Unnecessary categorical columns are dropped, and necessary columns (e.g. month as number) are added.\n    \"\"\"\n    try:\n        df = pd.read_csv(filename)\n        if SAVE_FIG is True:\n            # Visualize loaded time series data if applicable\n            df[\"date\"] = pd.to_datetime(df[\"date\"])\n            df.plot(x=\"date\", y=\"close\", figsize=(12,6), legend=None)\n            plt.xlabel(\"Time [Year]\")\n            plt.ylabel(\"Price [daily closing price in USD]\")\n            plt.title(df[\"Name\"][0] + \"-Stock Data 2013 to 2018\");\n            plt.savefig(\"./Plots/Stock-Price-Plots/\" + df[\"Name\"][0] + \"-Stock-Price-Plot.jpeg\")\n        \n        # Calculate base column for later class: future return of stock over given time horizon (e.g. this week's Monday to next week's Monday)\n        for horizon in time_horizons:\n            df[\"return_future_\" + str(horizon) + \"d\"] = (df[\"close\"].shift(-1*horizon)/df[\"close\"])-1\n        \n        # Convert date to numerical month to possibly detect cyclicality (e.g. christmas effect) in time series\n        df[\"month\"] = df[\"date\"].astype(\"datetime64[ns]\").dt.month\n        \n        if VERBOSE is True:\n            print(\"load_OHLC_data() -> Loaded DataFrame has the following columns:\")\n            for col in df:\n                print(\"Column \\'\" + col + \"\\' with type\", type(df[col][0]), \", e.g.\", df[col][0])\n            print(\"load_OHLC_data() -> df.head():\")\n            print(df.head())\n            \n        return df\n    except:\n        print(\"load_OHLC_data() -> Error, failed to find or load OHLC data from file with name \\'\"\n              + filename + \"\\'. Please provide well-formed CSV file with OHLC stock data\")",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# dfs_TECH_GROUP = []\n# for stock in TECH_GROUP:\n#     dfs_TECH_GROUP.append(load_OHLC_data(stock))",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.2 | Extract Features\n- Common metrics for technical analysis are calculated and added as features\n- Suitable TA libary for more technical features: https://github.com/bukosabino/ta"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def extract_OHLC_features(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Extract common technical stock analysis features from given OHLC stock data for distinct time horizons\n    \"\"\"\n    # Calculate technical features for each time horizon\n    for horizon in time_horizons:\n        # Past return of stock over given time horizon (e.g. last week's Monday to this week's Monday)\n        df[\"return_past_\" + str(horizon) + \"d\"] = (df[\"close\"]/df[\"close\"].shift(horizon))-1\n        \n        # Implied volatility measured by standard deviation\n        df[\"volatility_\" + str(horizon) + \"d\"] = df[\"close\"].rolling(horizon).std()\n        \n        # Moving averages (ma)\n        df[\"ma_\" + str(horizon) + \"d\"] = df[\"close\"].rolling(horizon).mean()\n        \n        # Momentum (absolute change in price over past horizon)\n        df[\"momentum_\" + str(horizon) + \"d\"] = df[\"close\"].diff(horizon)\n        \n        # If needed, use TA library for: ewma, #Pivot Points, Bollinger bands, Supports and Resistances, etc.\n    \n    # OHLC average is used for stock price average of a given day\n    df[\"ohlc_avg\"] = df[[\"open\", \"high\", \"low\", \"close\"]].mean(axis=1)\n    if VERBOSE is True:\n        print(\"extract_OHLC_features() -> extracted features, columns are now: \" + str(df.columns))\n    # Replace NaNs with zeroes\n    df = df.fillna(value=0)\n    \n    return df",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# for df in dfs_TECH_GROUP:\n#     df = extract_OHLC_features(df, TIME_HORIZONS)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## 1.3 | Anomaly Detection\n- Anomaly defined as: ABS(return_past_1d) > threshold=5% (default)\n- Such anomalies (5% threshold) occur in about 1.4% of instances for seven tech stock datasets"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "def detect_anomalies(df, threshold=0.05):\n    \"\"\"\n    Iterates through the DataFrame and prints out all dates where 1-day-return is greater than threshold=5% (default)\n    \"\"\"\n    if VERBOSE is True:\n        print(\"Detecting anomalies where abs(1-day-return)>\" + str(threshold*100) + \" % for \" + df[\"Name\"][0])\n\n    anomaly_counter = 0\n    for i in range(len(df)):\n        x = df[\"return_past_1d\"][i]\n        d = df[\"date\"][i]\n        if (abs(x) > threshold):\n            anomaly_counter = anomaly_counter + 1\n            if VERBOSE is True:\n                print(\"Anomaly: 1-day-return of \" + str(round(x * 100, 2)) + \"% on \" + d.strftime(\"%A, %d.%m.%Y\"))\n    \n    return anomaly_counter",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# anomaly_counter = 0\n# for df in dfs_TECH_GROUP:\n#     anomaly_counter = anomaly_counter + detect_anomalies(df, threshold=0.05)\n\n# print(\"anomaly_counter=\" + str(anomaly_counter) + \", or \" + str(round(anomaly_counter*100/(len(TECH_GROUP)*len(dfs_TECH_GROUP[0])), 2)) + \"% of instances\")",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.4 | Define classes\n- This notebook evaluates classifiers for stock recommendation where class indicates if stock goes up (class=1) or down (class=0) in given time horizon\n- Base columns used to determine these classes are subsequently removed to prevent illegal, future-peeking features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def define_classes(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Create target column in df: 1 means 'Yes, investor should buy stock', 0 means 'No, investor should not buy stock'.\n    The assumed trading strategy here is, that the investor buy the stock on a given date and sells it at the end of horizon.\n    Also removes illegal (future-peeking) columns\n    \"\"\"\n    for horizon in time_horizons:\n        base_column_name = \"return_future_\" + str(horizon) + \"d\"\n        class_name = \"class_\" + str(horizon) + \"d\"\n        \n        # 1. Add class column to dataframe\n        if class_name not in df.columns:\n            df[class_name] = np.where(df[base_column_name] > 0, 1, 0)\n        # 2. Remove base column as it would be an illegal (future-peeking) feature\n        if base_column_name in df.columns:\n            df = df.drop(columns=[base_column_name])\n    \n    return df",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for i in range(0, len(dfs_TECH_GROUP)):\n#     dfs_TECH_GROUP[i] = define_classes(dfs_TECH_GROUP[i])",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.5 | Check class balance"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def check_class_balance(df, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Plots class histogram for each time horizon provided\n    \"\"\"\n    for horizon in time_horizons:\n        class_name = \"class_\" + str(horizon) + \"d\"\n        if VERBOSE is True:\n            print(df[class_name].value_counts())\n        \n        if SAVE_FIG is True:\n            plt.figure()\n            df[class_name].hist()\n            plt.xlabel(\"Klasse\")\n            plt.ylabel(\"Haeufigkeit\")\n            plt.title(df[\"Name\"][0] + \"-Klassen-Histogramm-\" + str(horizon) + \"d\")\n            plt.savefig(\"./Plots/Class-Balance-Check/\" + df[\"Name\"][0] + \"-Class-Balance-Histogram-\" + str(horizon) + \"d.jpeg\")\n            plt.close()",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for i in range(0, len(dfs_TECH_GROUP)):\n#     check_class_balance(dfs_TECH_GROUP[i])",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.6 | Define training and test sets\n- Seed RandomState-s uniformly to make algorithms reproducible"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train_test_split_data(df, train_size=0.7, time_horizons=TIME_HORIZONS):\n    \"\"\"\n    Generates training and testing set from a given DataFrame dataset.\n    Assumes last COUNT(time_horizons) column(s) in DataFrame are classes, others are features\n    \"\"\"\n    # 1. Remove non-numerical features and save final feature names (e.g. for later matching to feature importances)\n    df = df.drop(columns=[\"Name\", \"date\"])\n    global g_feature_names\n    g_feature_names = df.columns.values.tolist()\n    \n    # 2. Split DataFrame into features (X) and target (y)\n    X = df.iloc[:,:-1*len(time_horizons)]\n    y = df.iloc[:,-1*len(time_horizons):]\n    \n    # 3. Use first (in same chronological order as time series) 70% to train and last 30% to test\n    split_index = int(len(X) * train_size)\n    if VERBOSE is True:\n        print(\"train_test_split_data() -> Split ist bei Index \" + str(split_index) + \" von \" + str(len(X)))\n    \n    X_train, X_test = X[:split_index], X[split_index:]\n    y_train, y_test = y[:split_index], y[split_index:]\n    return X_train, X_test, y_train, y_test",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.7 | Master function for loading data, extracting features and, if applicable, splitting data (train vs test)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_train_test_data(filename=MSFT_DATA, extract_features=True, horizon_index=2, train_size=0.7):\n    \"\"\"\n    Serves as a one-stop-shop function for data loading incl. preparation so that classifier only relies on this single function (and not multiple functions) \n    \"\"\"\n    # 1. Load data from .CSV (1.1)\n    df = load_OHLC_data(filename)\n    stock_ticker = df[\"Name\"][0]\n    if VERBOSE is True:\n        print(\"generate_train_test_data() -> loaded \" + str(stock_ticker))\n    \n    # 2. Extract features if applicable (1.2)\n    if extract_features is True:\n        df = extract_OHLC_features(df)\n    \n    # Detect anomalies (1.3) skipped\n    # 3. Define classes (1.4)\n    df = define_classes(df)\n    \n    # Check class balance (1.5) skipped\n    # 4. Split data into training and testing samples if applicable (or get 100% as training data to later apply CV) (1.6)\n    X_train, X_test, y_train, y_test = train_test_split_data(df, train_size=train_size)\n    \n    # 5. Return one single class series, depending on the horizon_index provided\n    y_train = y_train[y_train.columns[horizon_index]]\n    y_test = y_test[y_test.columns[horizon_index]]\n    \n    return stock_ticker, X_train, X_test, y_train, y_test",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(extract_features=False)\n# print(X_train.columns)\n# print(y_train.name)\n# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(extract_features=True)\n# print(X_train.columns)\n# print(y_train.name)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# 2 | Hyperparameter Tuning\n- Define tuninig functions for decision trees and random forests\n- These functions will later be invoked in the Final Evaluations stage"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.1 | Hyperparameter tuning a decision tree"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tune_decision_tree(dt, X, y, scoring=\"f1\"):\n    \"\"\"\n    Returns decision tree with best hyperparameters (out of all grid combinations)\n    \"\"\"\n    # 1. Create grid with all values that should be considered as hyperparameters\n    param_grid = {\"max_depth\": [3, 5, 7, 9, 11, 13, 15, 29],\n                      \"max_features\": [3, 5, 9 ,11, 13, 15, 29],\n                      \"min_samples_split\": [3, 5, 7, 10, 15, 29],\n                      \"min_samples_leaf\": [1, 3, 5, 7, 11, 29]}\n    \n    # 2. Run GridSearch. Randomized version is much faster with a small loss in optimality\n    tscv = TimeSeriesSplit(n_splits=10)\n    # grid_search = GridSearchCV(dt, param_grid, cv=tscv)\n    grid_search = RandomizedSearchCV(dt, param_grid, cv=tscv, random_state=RANDOM_SEED, scoring=scoring, n_jobs=-1)\n    grid_search.fit(X, y)\n\n    if VERBOSE is True:\n        print(\"tune_decision_tree() -> done.\")\n        print(\"tune_decision_tree() -> best score: \" + str(grid_search.best_score_ ))\n        print(\"tune_decision_tree() -> best params: \" + str(grid_search.best_params_ ))\n    \n    return grid_search.best_estimator_",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(AAPL_DATA, horizon_index=2, train_size=1)\nclf = DecisionTreeClassifier(random_state=RANDOM_SEED)\ndt_new = tune_decision_tree(clf, X_train, y_train)\n# print(\"dt_new ist: \" + str(dt_new))",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": "tune_decision_tree() -> done.\ntune_decision_tree() -> best score: 0.5529982832715039\ntune_decision_tree() -> best params: {'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 11, 'max_depth': 3}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Manual test: compare performance of best_params for scoring=accuracy vs scoring=f1"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(XRX_DATA, horizon_index=4, train_size=1)\nclf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n\ny_act, y_pred = apply_tscv(clf, X_train, y_train, return_predictions=True, tuning=False)\nprecision, recall, f1_measure = calculate_confusion_metrics(y_act, y_pred)\nprint(\"BEFORE TUNING: f1_measure=\" + str(f1_measure))\n\ny_act, y_pred = apply_tscv(clf, X_train, y_train, return_predictions=True, tuning=True)\nprecision, recall, f1_measure = calculate_confusion_metrics(y_act, y_pred)\nprint(\"AFTER TUNING: f1_measure=\" + str(f1_measure))",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": "BEFORE TUNING: f1_measure=0.4435401831129196\nAFTER TUNING: f1_measure=0.5350877192982456\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.2 | Hyperparameter tuning a random forest"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tune_random_forest(rf, X, y, scoring=\"f1\"):\n    \"\"\"\n    Returns random forest with best hyperparameters (out of all grid combinations)\n    \"\"\"\n    # 1. Create grid with all values that should be considered as hyperparameters\n    param_grid = {\"max_depth\": [3, 7, 17],\n                        \"min_samples_leaf\": [3, 13, 23]}\n#     rf.set_params(n_estimators=100) # Set n_estimators afterwards to reduce searching time\n\n    # 2. Run GridSearch. Randomized version is much faster with a small loss in optimality\n    tscv = TimeSeriesSplit(n_splits=10)\n    # grid_search = GridSearchCV(dt, param_grid, cv=tscv)\n    grid_search = RandomizedSearchCV(rf, param_grid, cv=tscv, random_state=RANDOM_SEED, n_iter=9, scoring=scoring)\n    grid_search.fit(X, y)\n\n    if VERBOSE is True:\n        print(\"tune_random_forest() -> done.\")\n        print(\"tune_random_forest() -> best score: \" + str(grid_search.best_score_ ))\n        print(\"tune_random_forest() -> best params: \" + str(grid_search.best_params_ ))\n    \n    return grid_search.best_estimator_",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(AAPL_DATA, horizon_index=2, train_size=1)\n# clf = RandomForestClassifier(n_estimators=5, random_state=RANDOM_SEED)\n# rf_new = tune_random_forest(clf, X_train, y_train)\n# print(\"rf_new ist: \" + str(rf_new))",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3 | Evaluation Stage\n- Define functions for building confusion matrixes, calculating performance metrics, determining feature importances and plotting findings\n- These functions will later be invoked in the Final Evaluations stage"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.1 | Apply time series cross validation (TSCV)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def apply_tscv(clf, X, y, return_predictions=False, tuning=False, importances_only=False):\n    \"\"\"\n    Uses time series cross validation to calculate feature importances, raw metrics (actuals, predicteds) or aggregated metrics (accuracy etc.)\n    \"\"\"\n    # 0. Instantiate TSCV and result variables\n    tscv = TimeSeriesSplit(n_splits=10)\n    train_sizes_manual = []\n    test_sizes_manual = []\n    train_scores_manual = []\n    test_scores_manual = []\n    y_actuals_raw = []\n    y_predicteds_raw = []\n    \n    # 1. Manually apply epoch indices from TSCV (as learning_curve() does not seem to work with TSCV, e.g. train_sizes is wrong)\n    epoch_counter = 0\n    for train_index, test_index in tscv.split(X):\n        # 2. Define indizes for training and testing\n        first_train_index = 0\n        last_train_index = train_index[-1]\n        first_test_index = test_index[0]\n        last_test_index = test_index[-1]\n        \n        global g_current_tscv_epoch\n        g_current_tscv_epoch = epoch_counter\n        if VERBOSE is True:\n            print(\"apply_tscv() -> epoch=\" + str(epoch_counter) + \", train on indices=[\" + str(first_train_index) + \n                  \", \" + str(last_train_index) + \"], test on indices=[\" + str(first_test_index) + \", \" + str(last_test_index) +\n                  \"], \" + \"i.e. train_size=\", len(train_index), \", test_size=\", len(test_index))\n        train_sizes_manual.extend([len(train_index)])\n        test_sizes_manual.extend([len(test_index)])\n        \n        # 3. Create training and test sets\n        X_train = X[first_train_index:last_train_index]\n        X_test = X[first_test_index:last_test_index]\n        y_train = y[first_train_index:last_train_index]\n        y_test = y[first_test_index:last_test_index]\n        \n        # 4. Delegate calculation of feature importances, raw metrics (actuals, predicteds) or aggregated metrics (accuracy)\n        if importances_only is True:\n            calculate_feature_importances(clf, X_train, X_test, y_train, y_test, tuning=tuning)\n        elif return_predictions is True: # return raw metrics (actuals, predicteds)\n            y_actual, y_predicted = get_actual_and_predicted_ys(clf, X_train, X_test, y_train, y_test, tuning=tuning)\n            # 5a. Add raw values to return list\n            y_actuals_raw.extend(y_actual)\n            y_predicteds_raw.extend(y_predicted)\n        else: # return aggregated metrics (accuracy)\n            train_score = calculate_accuracy(clf, X_train, X_train, y_train, y_train, tuning=False)\n            test_score = calculate_accuracy(clf, X_train, X_test, y_train, y_test, tuning=False)\n            # 5b. Add new scores to return list\n            train_scores_manual.extend([train_score])\n            test_scores_manual.extend([test_score])\n        \n        # 6. Increase epoch counter  \n        epoch_counter = epoch_counter + 1\n    \n    if importances_only is True:\n        return\n    elif return_predictions is True:\n        return y_actuals_raw, y_predicteds_raw\n    else:\n        return train_sizes_manual, train_scores_manual, test_scores_manual",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.2 | Calculate predictions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_actual_and_predicted_ys(clf, X_train, X_test, y_train, y_test, tuning=False):\n    \"\"\"\n    Generate predictions on given test set for further analysis (confusion metrics, accuracy, plotting, etc.)\n    \"\"\"\n    if not isinstance(y_train, pd.Series) or not isinstance(y_test, pd.Series):\n        print(\"get_actual_and_predicted_ys() -> y-DataFrames must have 1 column only.\")\n        return -1\n    \n    # 1. Fit and, if applicable, tune classifier to predict y-s\n    if tuning is True and isinstance(clf, type(DecisionTreeClassifier())):\n        clf = tune_decision_tree(clf, X_train, y_train)\n    elif tuning is True and isinstance(clf, type(RandomForestClassifier())):\n        clf = tune_random_forest(clf, X_train, y_train)\n        clf.set_params(n_estimators=100) # set n_estimators after GridSearch to reduce searching time\n    else:\n        clf.fit(X_train, y_train)\n    \n    # 2. Return actual and predicted classes on test set\n    y_actual = y_test\n    y_predicted = clf.predict(X_test)\n    \n    return y_actual, y_predicted",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.3 | Calculate feature importances"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def calculate_feature_importances(clf, X_train, X_test, y_train, y_test, tuning=False):\n    \"\"\"\n    Calculates feature importances and, if applicable, plots top10 features\n    \"\"\"\n    if not isinstance(y_train, pd.Series) or not isinstance(y_test, pd.Series):\n        print(\"calculate_feature_importances() -> y-DataFrames must have 1 column only.\")\n        return -1\n    \n    # 1. Fit and, if applicable, tune classifier to predict y-s\n    if tuning is True and isinstance(clf, type(DecisionTreeClassifier())):\n        clf = tune_decision_tree(clf, X_train, y_train)\n    elif tuning is True and isinstance(clf, type(RandomForestClassifier())):\n        clf = tune_random_forest(clf, X_train, y_train)\n        clf.set_params(n_estimators=100) # set n_estimators after GridSearch to reduce searching time\n    else:\n        clf.fit(X_train, y_train)\n    \n    # 2. Find and, if applicable, plot feature importances\n    feature_importances = clf.feature_importances_\n    global g_feature_importances # allow for global averaging (e.g. per horizon across all epochs)\n    g_feature_importances.append(feature_importances)\n\n    # 3. Find the top-10 features by importance\n    feature_ranking_indicies = np.argsort(feature_importances)\n    top10_features_indices = feature_ranking_indicies[-10:]\n    if VERBOSE is True:\n        print(\"calculate_feature_importances() -> Top1 feature:\" + str(g_feature_names[top10_features_indices[-1]]) + \" ->\" + str(feature_importances[top10_features_indices[-1]]))\n        print(\"calculate_feature_importances() -> Top2 feature:\" + str(g_feature_names[top10_features_indices[-2]]) + \" ->\" + str(feature_importances[top10_features_indices[-2]]))\n        print(\"calculate_feature_importances() -> Top3 feature:\" + str(g_feature_names[top10_features_indices[-3]]) + \" ->\" + str(feature_importances[top10_features_indices[-3]]))\n\n    # 4. Plot top10 feature importances in horizontal bars\n    fig = plt.figure()\n    plt.title(\"Einflussgrade der Features, \" + str(g_classifier) + \", AAPL\")\n    plt.barh(y=[g_feature_names[i] for i in top10_features_indices], width=[feature_importances[i] for i in top10_features_indices])\n    fig.tight_layout()\n    if SAVE_FIG is True:\n        plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Top10Importances-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \n                    \"-Epoch\" + str(g_current_tscv_epoch)+ \"-\" + str(g_stock_ticker) + \".jpeg\")\n    plt.close()\n    \n    return",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.4 | Calculate accuracy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def calculate_accuracy(clf, X_train, X_test, y_train, y_test, tuning=False):\n    \"\"\"\n    Calculates accuracy of non-tuned or tuned classifier\n    \"\"\"\n    if not isinstance(y_train, pd.Series) or not isinstance(y_test, pd.Series):\n        print(\"calculate_accuracy() -> y-DataFrames must have 1 column only.\")\n        return -1\n    \n    # 1. Fit and, if applicable, tune classifier to predict y-s\n    if tuning is True and isinstance(clf, type(DecisionTreeClassifier())):\n        clf = tune_decision_tree(clf, X_train, y_train)\n    elif tuning is True and isinstance(clf, type(RandomForestClassifier())):\n        clf = tune_random_forest(clf, X_train, y_train)\n        clf.set_params(n_estimators=100) # set n_estimators after GridSearch to reduce searching time\n    else:\n        clf.fit(X_train, y_train)\n    \n    # 2. Calculate and return accuracy\n    y_actual = y_test\n    y_predicted = clf.predict(X_test)\n    accuracy = metrics.accuracy_score(y_actual, y_predicted)\n    \n    return accuracy",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.5 | Calculate confusion metrics and, if applicable, plot confusion matrix"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def calculate_confusion_metrics(y_actual, y_predicted):\n    \"\"\"\n    Calculates precision, recall and f-measure and, if applicable, plots confusion matrix\n    \"\"\"\n    # 1. Calculate confusion metrics\n    precision = metrics.precision_score(y_actual, y_predicted) if (1 in y_predicted and 1 in y_actual) else -1\n    recall = metrics.recall_score(y_actual, y_predicted) if (1 in y_predicted and 1 in y_actual) else -1\n    f_measure = metrics.f1_score(y_actual, y_predicted) if (precision > 0 or recall >0) else -1\n    if VERBOSE is True:\n        print(\"calculate_confusion_metrics() -> Result on stock=\" + str(g_stock_ticker) + \" is precision=\" \n              + str(round(precision, 3)) + \", recall=\" + str(round(recall, 3)) + \", f_measure=\" + str(round(f_measure, 3)))\n    \n    # 2. Plot confusion matrix via Seaborn if applicable\n    confusion_matrix = metrics.confusion_matrix(y_actual, y_predicted)\n    fig, ax = plt.subplots()\n    sns_ax = sns.heatmap(confusion_matrix, annot=True, annot_kws={'size':15}, fmt='g', cmap=plt.cm.Blues, cbar=False, square=True)\n    sns_ax.invert_yaxis()\n    sns_ax.invert_xaxis()\n    ax.set(title= str(g_classifier) + \", \" + str(g_stock_ticker) + \", Horizont-Index=\" + str(g_horizon_index), ylabel=\"Korrekte Klasse\", xlabel=\"Vorhergesagte Klasse\")\n    if SAVE_FIG is True:\n        plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/ConfusionMatrix-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \"-\" + str(g_stock_ticker) + \".jpeg\")\n    plt.close()\n    \n    return precision, recall, f_measure",
      "execution_count": 74,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 4 | Visualization\n- Define plotting functions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.1 | Plot acuracy curve on train vs test data\n- E.g. for different max_depth-s of decision tree\n- Measure accuracy by TimeSeriesSplit"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_tscv_curve(clf, title, X, y):\n    \"\"\"\n    Plots the accuracies on training and testing set for given classifier using Time Series Cross Validation\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.ylim(0.00, 1.05)\n    \n    # 1. Apply Time Series CV to given data\n    train_sizes_manual, train_scores_manual, test_scores_manual = apply_tscv(clf, X, y)\n    \n    # 2. Plot data points for training and testing\n    if SAVE_FIG is True:\n        plt.plot(train_sizes_manual, train_scores_manual, 'o-', color=\"r\", label=\"Training\")\n        plt.plot(train_sizes_manual, test_scores_manual, 'o-', color=\"g\", label=\"Test\")\n        mean_test_acc = [np.mean(test_scores_manual)]*len(test_scores_manual)\n        plt.plot(train_sizes_manual, mean_test_acc, '--', color=\"g\", label=\"Mean\")\n        plt.text(0.5, 0.01, \"Mean=\" + str(round(np.mean(test_scores_manual), 3)), size=\"15\", weight=\"bold\", \n                 horizontalalignment=\"center\", verticalalignment=\"bottom\", transform=ax.transAxes)\n        plt.xlabel(\"Anzahl Trainingsinstanzen\")\n        plt.ylabel(\"Treffergenauigkeit\")\n        plt.title(title)\n        plt.grid()\n        plt.legend(loc=4)\n        plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Plot-\" + title.replace(\" \", \"\") + \".jpeg\")\n    \n    # 3. Return results for plotting results in master diagram with other clf's results\n    return train_sizes_manual, train_scores_manual, test_scores_manual",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 5 | Final Evaluations\n- In the following, above functions will be called on different datasets in order to evaluate classifiers (Dummy, DT, RF) in different settings\n \n## 5.0 | Evaluation settings\n- For flexible switching of settings (model, toggle feature extraction & tuning, etc.), the following cell serves as the single point of truth that all below functions are based on"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "evaluation_model = DecisionTreeClassifier(random_state=RANDOM_SEED) # DummyClassifier(RANDOM_SEED) vs DecisionTreeClassifier(RANDOM_SEED) vs RandomForestClassifier(RANDOM_SEED)\nevaluation_horizons_indices = list(range(0, len(TIME_HORIZONS))) # Which time horizon INDICES should be applied? E.g. list(range(0, len(TIME_HORIZONS))) for all horizon INDICES\nuse_feature_extraction = True # Extract features with technical analysis? True vs False\nuse_tuning = False # Tune hyperparameters of evaluation_model? True vs False\n\n# Define verbosity\nVERBOSE = False\n\n# Centrally define if figures should be saved to ./plots/\nSAVE_FIG = False",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Introduce global variables for easy plotting and easy averages across models/horizons/etc."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# These global strings should be set be the user. Will be used in .savefig() calls\ng_classifier = \"DT\" # This string will be included in the plot filenames, e.g. \"Tuned RF\", \"RF\", \"Dummy Classifier\", etc.\ng_plot_folder = \"Feature-Importances-TBD\" # Which subfolder of ./Plots is most suitable for planned plots? e.g. \"Feature-Importances-DT\"\n\n# These global variables are set programatically and do NOT need to be set in advance by the user. Useful for global calculations (e.g. averages)\ng_horizon_index = None # Will be included in the plot filenames, e.g. 2\ng_stock_ticker = None # Will be included in the plot filenames, e.g. \"AAPL\"\ng_feature_names = None # Useful for accessing feature names at any point at any time, e.g. when calculating feature importances\ng_current_tscv_epoch = None # Will be included in the plot filenames, e.g. 3\ng_feature_importances = [] # Useful for averaging feature importances independ of loop structures, e.g. average per horizon across all epochs",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Announce current settings ahead of evaluations"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def announce_eval_settings():\n    \"\"\"\n    Prints out key information about current evaluation settings\n    \"\"\"\n    print(\"announce_eval_settings() -> g_classifier=\" + str(g_classifier) + \" with type=\" + str(type(evaluation_model)))\n    print(\"announce_eval_settings() -> g_plot_folder=\" + str(g_plot_folder))\n    print(\"announce_eval_settings() -> use_feature_extraction=\" + str(use_feature_extraction) + \", use_tuning=\" + str(use_tuning))\n    print(\"announce_eval_settings() -> evaluation_horizons_indices=\" + str(evaluation_horizons_indices))\n    print(\"announce_eval_settings() -> SAVE_FIG=\" + str(SAVE_FIG) + \", VERBOSE=\" + str(VERBOSE))",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5.1 | Get confusion metrics via TSCV\n- Apply time series cross validation (tscv) to calculate confusion metrics for a given classifier on given stock for each time horizon"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_tscv_confusion_metrics(clf):\n    \"\"\"\n    Calculates confusion metrics for each stock (n=11) and for each horizon (n=6) for a total of 66 combinations plus TECH GROUP average\n    \"\"\"\n    announce_eval_settings()\n    # 0. Store interim results for later averaging across datasets\n    y_act_TechGroup = [[] for h in range(0, len(TIME_HORIZONS))]\n    y_pred_TechGroup = [[] for h in range(0, len(TIME_HORIZONS))]\n    stock_ticker = None\n    \n    # 1. Load and loop over stock datasets\n    for stock in [AAPL_DATA]:# TECH_GROUP:\n        print(\"get_tscv_results() -> Stock=\" + str(stock))\n        \n        # 2. Loop over prediction horizons\n        for horizon in evaluation_horizons_indices:\n            global g_horizon_index # make current horizon index accessible globally for .savefig()s\n            g_horizon_index = str(horizon)\n            \n            # 3. Load data for current stock-horizon combination. Train_size=100% because train-test splits will be executed by TSCV function\n            stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename=stock, extract_features=use_feature_extraction, horizon_index=horizon, train_size=1)\n            if horizon == 0: # make current stock ticker index accessible globally for .savefig()s\n                global g_stock_ticker \n                g_stock_ticker = str(stock_ticker)\n            \n            # 4. Get all TSCV predictions for confusion matrix and remember them for later averaging across all stocks\n            y_act, y_pred = apply_tscv(clf, X_train, y_train, return_predictions=True, tuning=use_tuning, importances_only=False)\n            y_act_TechGroup[horizon].extend(y_act)\n            y_pred_TechGroup[horizon].extend(y_pred)\n\n            # 5. Create confusion matrices and save figures if applicable\n            precision, recall, f_measure = calculate_confusion_metrics(y_act, y_pred)\n        \n    print(\"get_tscv_confusion_metrics() -> Stock=Tech Group Average\")\n    g_stock_ticker = str(\"Tech-Group-AVG\")\n    for horizon in  range(0, len(TIME_HORIZONS)):\n        g_horizon_index = str(horizon)\n        precision, recall, f_measure = calculate_confusion_metrics(y_act_TechGroup[horizon], y_pred_TechGroup[horizon])\n    \n    return",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# get_tscv_confusion_metrics(evaluation_model)",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5.2 | Get feature importances via TSCV\n- Apply time series cross validation (tscv) to calculate feature importances for a given classifier on given stock for each time horizon\n- Only works for decision tree and random forest, but not for dummy classifier"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_tscv_feature_importances(clf):\n    \"\"\"\n    Calculates feature importances of given classifier for each epoch (e.g. n=10) and for each horizon (e.g. n=6) for a total of 66 combinations\n    \"\"\"\n    announce_eval_settings()\n    # 1. Load and loop over stock datasets\n    for stock in [AAPL_DATA]:# TECH_GROUP:\n        print(\"get_tscv_feature_importances() -> Stock=\" + str(stock))\n        \n        # 2. Loop over prediction horizons\n        for horizon in evaluation_horizons_indices:\n            global g_horizon_index # make current horizon index accessible globally for .savefig()s\n            g_horizon_index = str(horizon)\n            \n            # 3. Load data for stock-horizon combination. Train_size=100% because train-test splits will be executed by TSCV function\n            stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename=stock, extract_features=use_feature_extraction, horizon_index=horizon, train_size=1)\n            if horizon == 0:\n                global g_stock_ticker # make current stock ticker index accessible globally for .savefig()s\n                g_stock_ticker = str(stock_ticker)\n            \n            # 4. Calculate feature importances\n            # Feature importances for each TSCV epoch within apply_tscv() are stored in the global variable g_feature_importances\n            apply_tscv(clf, X_train, y_train, tuning=use_tuning, importances_only=True)\n            # Here we calculate the average feature importances for current horizon (across all epochs). The importances for horizon-epoch combinations are covered in above .apply_tscv()\n            global g_feature_importances\n            # For every single feature, sum up the importances from all epochs. Afterwards, divivde by number of epochs to get the average importance of that feature\n            horizon_imps_sum = [0 for epoch in range(0, len(g_feature_importances[0]))]\n            for imps in g_feature_importances:\n                for i in range(0, len(imps)):\n                    horizon_imps_sum[i] = horizon_imps_sum[i] + imps[i]\n            horizon_imps_avg = [x/len(g_feature_importances) for x in horizon_imps_sum]\n\n            # 5. Plot horizontal bars of the average feature importances in current horizon (Top10)\n            feature_ranking_indicies = np.argsort(horizon_imps_avg)\n            top10_features_indices = feature_ranking_indicies[-10:]\n            fig = plt.figure()\n            plt.title(\"Einflussgrade der Features, \" + str(g_classifier) + \", AAPL\")\n            plt.barh(y=[g_feature_names[i] for i in top10_features_indices], width=[horizon_imps_avg[i] for i in top10_features_indices])\n            fig.tight_layout()\n            if SAVE_FIG is True:\n                plt.savefig(\"./Plots/\" + str(g_plot_folder) + \"/Top10Importances-\" + str(g_classifier) + \"-HorizonIndex\" + str(g_horizon_index) + \n                            \"-EpochsAVG-\" + str(g_stock_ticker) + \".jpeg\")\n            plt.close()\n            # Reset feature importances for next horizon\n            g_feature_importances = []\n    \n    return",
      "execution_count": 75,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# get_tscv_feature_importances(evaluation_model)",
      "execution_count": 76,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## 5.3 | TBD (additional result besides confusion metrics and feature importances needed?)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5.TBD | Decision tree hyperparameter variation example (max_depth)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_dt(filename=AAPL_DATA, extract_features=True, horizon_index=2):\n    \"\"\"\n    TBD\n    \"\"\"\n    announce_eval_settings()\n    # Load data\n    stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename, extract_features, horizon_index, train_size=1)\n\n    # Generate classifier\n    clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n\n    # Gather all plotted series to create master plot\n    plotted_Xs = []\n    plotted_ys_train = []\n    plotted_ys_test = []\n\n    # Evaluate classifier for different values of selected hyperparameter\n    for i in [1, 3, 5, 6, 7, 9, 11, None]:\n        clf.set_params(max_depth=i)\n        X_res, y_train_res, y_test_res = plot_tscv_curve(clf, \"Decision Tree Treffergenauigkeiten AAPL (MAX_DEPTH=\" + str(i) + \")\", X_train, y_train)\n        print(\"Mean test accuracy for MAX_DEPTH=\" + str(i) + \": \" + str(round(np.mean(y_test_res), 3)))\n        \n#         # Throw best params from 2.2 (Hyperparameter tuning) in and see if same result\n#         if i == 7:\n#             print(\"i is 7, so here we try the GRID_SEARCHED-best params: 'max_depth': 29, 'max_features': 29, 'min_samples_leaf': 1, 'min_samples_split': 15}, achieved 58.3% acc above\")\n#             clf_best_params = DecisionTreeClassifier(random_state=RANDOM_SEED, max_depth=29, max_features=29, min_samples_leaf=1, min_samples_split=15)\n#             X_res, y_train_res, y_test_res = plot_tscv_curve(clf_best_params, \"Decision Tree Treffergenauigkeiten AAPL (BEST_PARAMS)\", X_train, y_train)\n#             print(\"Best-params DT achievend ACC=\" + str(i) + \": \" + str(round(np.mean(y_test_res), 3)))\n        \n        # TBD: Use all results to make master plot\n        plotted_Xs.extend([X_res])\n        plotted_ys_train.extend([y_train_res])\n        plotted_ys_test.extend([y_test_res])",
      "execution_count": 155,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# evaluate_dt(filename=AAPL_DATA, extract_features=True, horizon_index=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5.TBD | Random forest hyperparameter variation example (n_estimators)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate_rf(filename=AAPL_DATA, extract_features=True, horizon_index=2):\n    \"\"\"\n    TBD\n    \"\"\"\n    announce_eval_settings()\n    # Load data\n    stock_ticker, X_train, X_test, y_train, y_test = generate_train_test_data(filename, extract_features, horizon_index, train_size=1)\n\n    # Generate classifier\n    clf = RandomForestClassifier(n_estimators = 1, random_state=RANDOM_SEED)\n\n    # Evaluate classifier for different values of selected hyperparameter\n    for i in [1, 5, 10, 50, 100]:\n        clf.set_params(n_estimators=i)\n        X_res, y_train_res, y_test_res = plot_tscv_curve(clf, \"Random Forest Treffergenauigkeiten AAPL (N_ESTIMATORS=\" + str(i) + \")\", X_train, y_train)\n        print(\"Mean test accuracy for N_ESTIMATORS=\" + str(i) + \": \" + str(round(np.mean(y_test_res), 3)))\n    \n    # TBD: Plotting",
      "execution_count": 102,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# evaluate_rf(filename=MSFT_DATA, extract_features=True, horizon_index=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 6. Reproducing Thesis Results"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TBD: Alle ERgebnisplots hier mit (1) Settings anpassen und (2) Ploten als Skritp hinterlgeegn für Verifizierung",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}